{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge xgboost\n",
    "\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn joblib pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import yaml\n",
    "import logging\n",
    "from typing import Dict, Tuple, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertPredictionSystem:\n",
    "    def __init__(self, config_path: str = 'config.yaml'):\n",
    "        \"\"\"Initialize the Alert Prediction System with configuration.\"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.setup_logging()\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.best_models = {}\n",
    "        self.feature_importances = {}\n",
    "\n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        default_config = {\n",
    "            'prediction_window': 7,\n",
    "            'probability_threshold': 0.7,\n",
    "            'test_size': 0.2,\n",
    "            'validation_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'model_params': {\n",
    "                'RandomForest': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10]\n",
    "                },\n",
    "                'XGBoost': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.1]\n",
    "                },\n",
    "                'GradientBoosting': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 5],\n",
    "                    'learning_rate': [0.01, 0.1]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            return {**default_config, **config}\n",
    "        except FileNotFoundError:\n",
    "            return default_config\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging settings.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('alert_prediction.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_and_preprocess_data(self, folder: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and preprocess data from multiple HTOL machines.\"\"\"\n",
    "        self.logger.info(\"Loading and preprocessing data...\")\n",
    "\n",
    "        dfs = []\n",
    "        for i in range(9, 16):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            try:\n",
    "                file = os.path.join(folder, file_name)\n",
    "                print(file)\n",
    "                df = pd.read_csv(file)\n",
    "                df['machine_id'] = f'HTOL-{i:02d}'\n",
    "                dfs.append(df)\n",
    "            except FileNotFoundError:\n",
    "                self.logger.warning(f\"File not found: {file_name}\")\n",
    "                continue\n",
    "\n",
    "        if not dfs:\n",
    "            raise ValueError(\"No data files were successfully loaded\")\n",
    "\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "        print(combined_df.head())\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Engineer features for the model.\"\"\"\n",
    "        self.logger.info(\"Engineering features...\")\n",
    "\n",
    "        # Time-based features\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['time_of_day'] = pd.cut(df['hour'],\n",
    "                                 bins=[0, 6, 12, 18, 24],\n",
    "                                 labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "        # Rolling statistics with different windows\n",
    "        windows = [12, 24, 48]  # hours\n",
    "        for window in windows:\n",
    "            df[f'rolling_mean_{window}h'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "                window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "            df[f'rolling_std_{window}h'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "                window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Alert history features\n",
    "        alert_types = ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']\n",
    "        for alert_type in alert_types:\n",
    "            # Time since last alert\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "            ).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "\n",
    "            # Alert frequency in last week\n",
    "            df[f'{alert_type}_freq_7d'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['ALERT'].eq(alert_type).rolling('7D', on='Time').sum()\n",
    "            ).reset_index(level=0, drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_for_classification(self, df: pd.DataFrame, target_alert_type: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Prepare data for classification.\"\"\"\n",
    "        self.logger.info(f\"Preparing data for {target_alert_type} alert prediction...\")\n",
    "\n",
    "        # Create target variable\n",
    "        df['target'] = df.groupby('machine_id').apply(\n",
    "            lambda x: (x['ALERT'] == target_alert_type)\n",
    "            .rolling(window=self.config['prediction_window'])\n",
    "            .max()\n",
    "            .shift(-self.config['prediction_window']+1)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Select features\n",
    "        feature_cols = [\n",
    "            'ChlPrs', 'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "            *[f'rolling_mean_{w}h' for w in [12, 24, 48]],\n",
    "            *[f'rolling_std_{w}h' for w in [12, 24, 48]],\n",
    "            *[f'time_since_{at}' for at in ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']],\n",
    "            *[f'{at}_freq_7d' for at in ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']]\n",
    "        ]\n",
    "\n",
    "        X = df[feature_cols].copy()\n",
    "        y = df['target'].fillna(0)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"Split data into train, validation, and test sets using time-based splitting.\"\"\"\n",
    "        self.logger.info(\"Splitting data into train, validation, and test sets...\")\n",
    "\n",
    "        # First split: train + validation vs test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.config['test_size'],\n",
    "            random_state=self.config['random_state'],\n",
    "            shuffle=False  # Time-based split\n",
    "        )\n",
    "\n",
    "        # Second split: train vs validation\n",
    "        validation_size = self.config['validation_size'] / (1 - self.config['test_size'])\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=validation_size,\n",
    "            random_state=self.config['random_state'],\n",
    "            shuffle=False  # Time-based split\n",
    "        )\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def train_and_evaluate_models(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame,\n",
    "                                y_train: pd.Series, y_val: pd.Series, y_test: pd.Series, alert_type: str) -> Dict:\n",
    "        \"\"\"Train and evaluate multiple models.\"\"\"\n",
    "        self.logger.info(f\"Training and evaluating models for {alert_type} alerts...\")\n",
    "\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(),\n",
    "            'XGBoost': XGBClassifier(),\n",
    "            'GradientBoosting': GradientBoostingClassifier()\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            self.logger.info(f\"Training {model_name}...\")\n",
    "\n",
    "            # Train model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_val_pred = model.predict(X_val_scaled)\n",
    "            y_val_prob = model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "            # Calculate metrics\n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'validation_report': classification_report(y_val, y_val_pred, output_dict=True),\n",
    "                'confusion_matrix': confusion_matrix(y_val, y_val_pred),\n",
    "                'roc_auc': auc(roc_curve(y_val, y_val_prob)[0], roc_curve(y_val, y_val_prob)[1])\n",
    "            }\n",
    "\n",
    "            # Store feature importances for tree-based models\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                results[model_name]['feature_importances'] = dict(zip(X_train.columns,\n",
    "                                                                    model.feature_importances_))\n",
    "\n",
    "        # Select best model based on ROC AUC\n",
    "        best_model_name = max(results.items(), key=lambda x: x[1]['roc_auc'])[0]\n",
    "        best_model = results[best_model_name]['model']\n",
    "\n",
    "        # Evaluate best model on test set\n",
    "        y_test_pred = best_model.predict(X_test_scaled)\n",
    "        y_test_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        results['best_model'] = {\n",
    "            'name': best_model_name,\n",
    "            'model': best_model,\n",
    "            'test_report': classification_report(y_test, y_test_pred, output_dict=True),\n",
    "            'test_confusion_matrix': confusion_matrix(y_test, y_test_pred),\n",
    "            'test_roc_auc': auc(roc_curve(y_test, y_test_prob)[0], roc_curve(y_test, y_test_prob)[1])\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def visualize_results(self, results: Dict, alert_type: str):\n",
    "        \"\"\"Visualize model results.\"\"\"\n",
    "        self.logger.info(f\"Visualizing results for {alert_type} alerts...\")\n",
    "\n",
    "        # Create a figure with multiple subplots\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # 1. ROC curves comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        for model_name, result in results.items():\n",
    "            if model_name != 'best_model':\n",
    "                fpr, tpr, _ = roc_curve(y_val, result['model'].predict_proba(X_val_scaled)[:, 1])\n",
    "                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {result[\"roc_auc\"]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves Comparison')\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Feature Importance\n",
    "        plt.subplot(2, 2, 2)\n",
    "        best_model_name = results['best_model']['name']\n",
    "        if 'feature_importances' in results[best_model_name]:\n",
    "            importances = pd.Series(results[best_model_name]['feature_importances']).sort_values(ascending=True)\n",
    "            importances.plot(kind='barh')\n",
    "            plt.title(f'Feature Importance ({best_model_name})')\n",
    "\n",
    "        # 3. Confusion Matrix\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.heatmap(results['best_model']['test_confusion_matrix'],\n",
    "                   annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - Test Set ({best_model_name})')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "\n",
    "        # 4. Precision-Recall Trade-off\n",
    "        plt.subplot(2, 2, 4)\n",
    "        metrics = results['best_model']['test_report']\n",
    "        plt.bar(['Precision', 'Recall', 'F1-Score'],\n",
    "                [metrics['1']['precision'], metrics['1']['recall'], metrics['1']['f1-score']])\n",
    "        plt.title(f'Precision-Recall Metrics - Test Set ({best_model_name})')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'model_evaluation_{alert_type}.png')\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self, model, scaler, alert_type: str):\n",
    "        \"\"\"Save the trained model and scaler.\"\"\"\n",
    "        self.logger.info(f\"Saving model for {alert_type} alerts...\")\n",
    "\n",
    "        model_path = f'models/model_{alert_type}.joblib'\n",
    "        scaler_path = f'models/scaler_{alert_type}.joblib'\n",
    "\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        joblib.dump(model, model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    def load_model(self, alert_type: str) -> Tuple[Any, Any]:\n",
    "        \"\"\"Load a trained model and scaler.\"\"\"\n",
    "        model_path = f'models/model_{alert_type}.joblib'\n",
    "        scaler_path = f'models/scaler_{alert_type}.joblib'\n",
    "\n",
    "        model = joblib.load(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "\n",
    "        return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlertPredictionAPI:\n",
    "    \"\"\"API class for making predictions in production.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir: str = 'models'):\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load all trained models and scalers.\"\"\"\n",
    "        alert_types = ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']\n",
    "        for alert_type in alert_types:\n",
    "            try:\n",
    "                model_path = os.path.join(self.model_dir, f'model_{alert_type}.joblib')\n",
    "                scaler_path = os.path.join(self.model_dir, f'scaler_{alert_type}.joblib')\n",
    "\n",
    "                self.models[alert_type] = joblib.load(model_path)\n",
    "                self.scalers[alert_type] = joblib.load(scaler_path)\n",
    "            except FileNotFoundError as e:\n",
    "                logging.error(f\"Failed to load model for {alert_type}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def preprocess_data(self, data: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess incoming data for prediction.\"\"\"\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Convert timestamp if provided as string\n",
    "        if 'Time' in df.columns and isinstance(df['Time'].iloc[0], str):\n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "        # Extract time-based features\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Ensure all required features are present\n",
    "        required_features = [\n",
    "            'ChlPrs', 'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "            *[f'rolling_mean_{w}h' for w in [12, 24, 48]],\n",
    "            *[f'rolling_std_{w}h' for w in [12, 24, 48]],\n",
    "            *[f'time_since_{at}' for at in ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']],\n",
    "            *[f'{at}_freq_7d' for at in ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']]\n",
    "        ]\n",
    "\n",
    "        missing_features = set(required_features) - set(df.columns)\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "\n",
    "        return df[required_features]\n",
    "\n",
    "    def predict(self, data: Dict, threshold: float = 0.7) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Make predictions for all alert types.\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary containing the required features\n",
    "            threshold: Probability threshold for considering an alert likely\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with predictions for each alert type\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_data = self.preprocess_data(data)\n",
    "            predictions = {}\n",
    "\n",
    "            for alert_type in self.models:\n",
    "                # Scale features\n",
    "                scaled_data = self.scalers[alert_type].transform(processed_data)\n",
    "\n",
    "                # Get probability predictions\n",
    "                prob = self.models[alert_type].predict_proba(scaled_data)[0, 1]\n",
    "\n",
    "                predictions[alert_type] = {\n",
    "                    'probability': float(prob),\n",
    "                    'likely_alert': bool(prob > threshold)\n",
    "                }\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction error: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 18:18:44,807 - INFO - Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-09_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-10_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-11_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-12_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-13_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-14_alerts.csv\n",
      "../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01/HTOL-15_alerts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 18:18:46,450 - INFO - Engineering features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Time  ChlPrs  alert_index ALERT                   file_name  \\\n",
      "0 2024-03-14 09:50:49   32.66            0   NaN  HTOL-09-20240314095049.csv   \n",
      "1 2024-03-14 09:50:49   32.63            1   NaN  HTOL-09-20240314095049.csv   \n",
      "2 2024-03-14 09:50:50   32.58            2   NaN  HTOL-09-20240314095049.csv   \n",
      "3 2024-03-14 09:50:51   32.69            3   NaN  HTOL-09-20240314095049.csv   \n",
      "4 2024-03-14 09:50:53   32.62            4   NaN  HTOL-09-20240314095049.csv   \n",
      "\n",
      "  machine_id  \n",
      "0    HTOL-09  \n",
      "1    HTOL-09  \n",
      "2    HTOL-09  \n",
      "3    HTOL-09  \n",
      "4    HTOL-09  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 18:18:48,431 - ERROR - Error in main execution: invalid on specified as Time, must be a column (of DataFrame), an Index or None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid on specified as Time, must be a column (of DataFrame), an Index or None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load and process data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     df \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39mload_and_preprocess_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Train models for each alert type\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     alert_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDIUM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIGH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSIGMA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[36], line 115\u001b[0m, in \u001b[0;36mAlertPredictionSystem.engineer_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    112\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Alert frequency in last week\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_freq_7d\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmachine_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALERT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43malert_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m7D\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/groupby/groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[1;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1829\u001b[0m         ):\n\u001b[1;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1836\u001b[0m             )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/groupby/groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/groupby/ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 116\u001b[0m, in \u001b[0;36mAlertPredictionSystem.engineer_features.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    112\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Alert frequency in last week\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_freq_7d\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALERT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43malert_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m7D\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    117\u001b[0m     )\u001b[38;5;241m.\u001b[39mreset_index(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/generic.py:12580\u001b[0m, in \u001b[0;36mNDFrame.rolling\u001b[0;34m(self, window, min_periods, center, win_type, on, axis, closed, step, method)\u001b[0m\n\u001b[1;32m  12566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m win_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m  12567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Window(\n\u001b[1;32m  12568\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12569\u001b[0m         window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12577\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m  12578\u001b[0m     )\n\u001b[0;32m> 12580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRolling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12581\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12587\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/window/rolling.py:164\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, obj, window, min_periods, center, win_type, axis, on, closed, step, method, selection)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon])\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid on specified as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be a column (of DataFrame), an Index or None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;241m=\u001b[39m selection\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate()\n",
      "\u001b[0;31mValueError\u001b[0m: invalid on specified as Time, must be a column (of DataFrame), an Index or None"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the Alert Prediction System.\"\"\"\n",
    "\n",
    "    # Initialize and train the system\n",
    "    system = AlertPredictionSystem()\n",
    "\n",
    "    try:\n",
    "        # Load and process data\n",
    "        df = system.load_and_preprocess_data(\"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\")\n",
    "        df = system.engineer_features(df)\n",
    "\n",
    "        # Train models for each alert type\n",
    "        alert_types = ['LOW', 'MEDIUM', 'HIGH', 'SIGMA']\n",
    "        for alert_type in alert_types:\n",
    "            # Prepare data\n",
    "            X, y = system.prepare_data_for_classification(df, alert_type)\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = system.split_data(X, y)\n",
    "\n",
    "            # Train and evaluate models\n",
    "            results = system.train_and_evaluate_models(\n",
    "                X_train, X_val, X_test,\n",
    "                y_train, y_val, y_test,\n",
    "                alert_type\n",
    "            )\n",
    "\n",
    "            # Visualize results\n",
    "            system.visualize_results(results, alert_type)\n",
    "\n",
    "            # Save the best model\n",
    "            best_model = results['best_model']['model']\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            system.save_model(best_model, scaler, alert_type)\n",
    "\n",
    "        # Initialize the API for production use\n",
    "        api = AlertPredictionAPI()\n",
    "\n",
    "        # Example prediction\n",
    "        sample_data = {\n",
    "            'Time': datetime.now(),\n",
    "            'ChlPrs': 2.5,\n",
    "            'rolling_mean_12h': 2.4,\n",
    "            'rolling_mean_24h': 2.3,\n",
    "            'rolling_mean_48h': 2.2,\n",
    "            'rolling_std_12h': 0.1,\n",
    "            'rolling_std_24h': 0.2,\n",
    "            'rolling_std_48h': 0.3,\n",
    "            'time_since_LOW': 48.0,\n",
    "            'time_since_MEDIUM': 72.0,\n",
    "            'time_since_HIGH': 96.0,\n",
    "            'time_since_SIGMA': 120.0,\n",
    "            'LOW_freq_7d': 1,\n",
    "            'MEDIUM_freq_7d': 0,\n",
    "            'HIGH_freq_7d': 0,\n",
    "            'SIGMA_freq_7d': 0\n",
    "        }\n",
    "\n",
    "        predictions = api.predict(sample_data)\n",
    "        print(\"\\nPredictions for sample data:\")\n",
    "        for alert_type, pred in predictions.items():\n",
    "            print(f\"{alert_type}: Probability = {pred['probability']:.2f}, \"\n",
    "                  f\"Likely Alert = {pred['likely_alert']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if True:\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
