{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertPredictor:\n",
    "    def __init__(self, model_type='xgboost'):\n",
    "        \"\"\"\n",
    "        Initializes the AlertPredictor with the specified model type ('xgboost' or 'randomforest').\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH', ] # 'SIGMA']\n",
    "        self.features = ['ChlPrs',\n",
    "                         # 'hour',\n",
    "                         # 'day_of_week',\n",
    "                         # 'month',\n",
    "                         # 'is_weekend',\n",
    "                         'rolling_mean', 'rolling_std'] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses data from CSV files in the specified folder.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for i in range(9, 16):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            df = pd.read_csv(os.path.join(folder, file_name))\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineers features from the preprocessed data.\n",
    "        \"\"\"\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Calculate time since last alert for each type\n",
    "        for alert_type in self.alert_types:\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_for_classification(self, df, target_alert_type, prediction_window):\n",
    "        \"\"\"\n",
    "        Prepares the data for training the classification model.\n",
    "        \"\"\"\n",
    "        df['target'] = df.groupby('machine_id').apply(\n",
    "            lambda x: (x['ALERT'] == target_alert_type).rolling(window=prediction_window).max().shift(-prediction_window + 1)).reset_index(level=0,\n",
    "                                                                                                                                         drop=True)\n",
    "\n",
    "        X = df[self.features]\n",
    "        y = df['target'].fillna(0)  # Fill NaN with 0 (no alert)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Trains and evaluates the classification model.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            # XGBoost configuration for imbalanced classification\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  # Handle class imbalance\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                early_stopping_rounds=10,\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=0)\n",
    "        elif self.model_type == 'randomforest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'xgboost' or 'randomforest'.\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder, prediction_window=7):\n",
    "        \"\"\"\n",
    "        Trains the models for each alert type.\n",
    "        \"\"\"\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\nTraining model for {alert_type} alerts:\")\n",
    "            X, y = self.prepare_data_for_classification(df, alert_type, prediction_window)\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Makes predictions on new data.\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        for alert_type in self.alert_types:\n",
    "            X_new = new_data[self.features]\n",
    "            X_new_scaled = self.scalers[alert_type].transform(X_new)\n",
    "            alert_probability = self.models[alert_type].predict_proba(X_new_scaled)[0, 1]\n",
    "            predictions[alert_type] = alert_probability\n",
    "        return predictions\n",
    "\n",
    "    def visualize_alerts(self, df, target_alert_type, prediction_window, probability_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Visualizes actual alerts and high-risk periods.\n",
    "        \"\"\"\n",
    "        X = df[self.features]\n",
    "        X_scaled = self.scalers[target_alert_type].transform(X)\n",
    "\n",
    "        df['alert_probability'] = self.models[target_alert_type].predict_proba(X_scaled)[:, 1]\n",
    "        df['high_risk'] = df['alert_probability'] > probability_threshold\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        machines = df['machine_id'].unique()\n",
    "        n_machines = len(machines)\n",
    "\n",
    "        for i, machine_id in enumerate(machines):\n",
    "            machine_df = df[df['machine_id'] == machine_id]\n",
    "\n",
    "            # Plot actual alerts\n",
    "            alerts = machine_df[machine_df['ALERT'] == target_alert_type]\n",
    "            plt.scatter(alerts['Time'], [i - 0.2] * len(alerts), marker='o', s=100,\n",
    "                        label=f'Actual {target_alert_type} Alert' if i == 0 else \"\")\n",
    "\n",
    "            # Plot high-risk periods\n",
    "            high_risk_periods = machine_df[machine_df['high_risk']]\n",
    "            plt.scatter(high_risk_periods['Time'], [i + 0.2] * len(high_risk_periods), marker='x', s=100, label=f'High Risk Period ({target_alert_type})' if i == 0 else \"\")\n",
    "\n",
    "            plt.text(df['Time'].min(), i, machine_id, va='center', ha='right', fontweight='bold')\n",
    "\n",
    "        plt.yticks(range(n_machines), machines)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Machine ID')\n",
    "        plt.title(f'Actual Alerts vs High Risk Periods for {target_alert_type} Alerts')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionAlertPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the production predictor that handles both XGBoost and Random Forest models.\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.features = [\n",
    "            'ChlPrs',\n",
    "            # 'hour',\n",
    "            # 'day_of_week',\n",
    "            # 'month',\n",
    "            # 'is_weekend',\n",
    "            'rolling_mean',\n",
    "            'rolling_std'\n",
    "        ] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def save_models(self, xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                    save_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Save trained models and scalers to disk.\n",
    "\n",
    "        Args:\n",
    "            xgb_predictor: Trained XGBoost AlertPredictor instance\n",
    "            rf_predictor: Trained Random Forest AlertPredictor instance\n",
    "            save_dir: Directory to save the models\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'features': self.features,\n",
    "            'alert_types': self.alert_types,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        joblib.dump(metadata, os.path.join(save_dir, 'metadata.joblib'))\n",
    "\n",
    "        # Save models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Save XGBoost models and scalers\n",
    "            joblib.dump(\n",
    "                xgb_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                xgb_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Save Random Forest models and scalers\n",
    "            joblib.dump(\n",
    "                rf_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                rf_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def load_models(self, load_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Load saved models and scalers from disk.\n",
    "\n",
    "        Args:\n",
    "            load_dir: Directory containing the saved models\n",
    "        \"\"\"\n",
    "        # Load metadata\n",
    "        metadata = joblib.load(os.path.join(load_dir, 'metadata.joblib'))\n",
    "        self.features = metadata['features']\n",
    "        self.alert_types = metadata['alert_types']\n",
    "\n",
    "        # Initialize nested dictionaries for models and scalers\n",
    "        self.models = {'xgboost': {}, 'randomforest': {}}\n",
    "        self.scalers = {'xgboost': {}, 'randomforest': {}}\n",
    "\n",
    "        # Load models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Load XGBoost\n",
    "            self.models['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Load Random Forest\n",
    "            self.models['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare features for prediction.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing at minimum 'Time', 'ChlPrs', and 'machine_id' columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with engineered features\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "\n",
    "        # Time-based features\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Time since last alert features\n",
    "        for alert_type in self.alert_types:\n",
    "            if 'ALERT' in df.columns:\n",
    "                df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                    lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "                ).reset_index(level=0, drop=True)\n",
    "                df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "            else:\n",
    "                # For new data without alert history, use a large value\n",
    "                df[f'time_since_{alert_type}'] = 168  # One week in hours\n",
    "\n",
    "        return df[self.features]\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Make predictions using the loaded models.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing required features\n",
    "            model_type: 'xgboost' or 'randomforest'\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing predictions for each machine and alert type\n",
    "        \"\"\"\n",
    "        if model_type not in ['xgboost', 'randomforest']:\n",
    "            raise ValueError(\"model_type must be 'xgboost' or 'randomforest'\")\n",
    "\n",
    "        # Prepare features\n",
    "        X = self.prepare_features(data)\n",
    "\n",
    "        # Make predictions for each machine and alert type\n",
    "        predictions = {}\n",
    "        for machine_id in data['machine_id'].unique():\n",
    "            machine_data = X[data['machine_id'] == machine_id]\n",
    "            machine_predictions = {}\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                # Scale the features\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(machine_data)\n",
    "\n",
    "                # Get prediction probabilities\n",
    "                probs = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "                # Store the average probability for this alert type\n",
    "                machine_predictions[alert_type] = float(probs.mean())\n",
    "\n",
    "            predictions[machine_id] = machine_predictions\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def save_trained_models(xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                        save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Convenience function to save trained models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.save_models(xgb_predictor, rf_predictor, save_dir)\n",
    "    print(f\"Models saved successfully to {save_dir}\")\n",
    "\n",
    "def load_production_predictor(load_dir: str) -> ProductionAlertPredictor:\n",
    "    \"\"\"\n",
    "    Convenience function to load saved models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.load_models(load_dir)\n",
    "    return production_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnsembleAlertPredictor:\n",
    "    def __init__(self, base_predictor: ProductionAlertPredictor,\n",
    "                 lookback_window: int = 168,  # 7 days in hours\n",
    "                 prediction_window: int = 168,  # 7 days in hours\n",
    "                 agreement_threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the ensemble predictor that requires agreement between XGBoost and Random Forest.\n",
    "        \"\"\"\n",
    "        self.predictor = base_predictor\n",
    "        self.lookback_window = lookback_window\n",
    "        self.prediction_window = prediction_window\n",
    "        self.agreement_threshold = agreement_threshold\n",
    "\n",
    "    def prepare_time_series_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare time series features from historical data.\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "\n",
    "        # Ensure numeric type for ChlPrs\n",
    "        df['ChlPrs'] = pd.to_numeric(df['ChlPrs'], errors='coerce')\n",
    "\n",
    "        # Convert Time to datetime if it's not already\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "        # Sort the data\n",
    "        df = df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        # Initialize features dictionary\n",
    "        feature_dict = {\n",
    "            'ChlPrs': df['ChlPrs'].copy(),  # Create a copy to avoid SettingWithCopyWarning\n",
    "            'machine_id': df['machine_id'].copy(),  # Create a copy to avoid SettingWithCopyWarning\n",
    "            'Time': df['Time'].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "        }\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        for machine in df['machine_id'].unique():\n",
    "            mask = df['machine_id'] == machine\n",
    "            # Use .transform to ensure the output is the same length as the original\n",
    "            feature_dict['rolling_mean'] = df.loc[mask, 'ChlPrs'].rolling(\n",
    "                window=24, min_periods=1).mean().transform(lambda x: x.fillna(method='bfill')).values\n",
    "            feature_dict['rolling_std'] = df.loc[mask, 'ChlPrs'].rolling(\n",
    "                window=24, min_periods=1).std().transform(lambda x: x.fillna(method='bfill')).values\n",
    "\n",
    "        # Handle time since last alert features\n",
    "        if 'ALERT' in df.columns:\n",
    "            for alert_type in self.predictor.alert_types:\n",
    "                # Initialize as an array of NaNs with the same length as the DataFrame\n",
    "                feature_dict[f'time_since_{alert_type}'] = np.full(len(df), np.nan)\n",
    "                for machine in df['machine_id'].unique():\n",
    "                    mask = df['machine_id'] == machine\n",
    "                    machine_data = df[mask].copy()\n",
    "                    alert_times = machine_data[machine_data['ALERT'] == alert_type]['Time']\n",
    "                    if not alert_times.empty:\n",
    "                        last_alert = alert_times.shift(1)\n",
    "                        hours_since = (machine_data['Time'] - last_alert).dt.total_seconds() / 3600\n",
    "                        # Assign the values to the array using boolean indexing\n",
    "                        feature_dict[f'time_since_{alert_type}'][mask] = hours_since.values\n",
    "        else:\n",
    "            for alert_type in self.predictor.alert_types:\n",
    "                feature_dict[f'time_since_{alert_type}'] = 168.0  # One week in hours\n",
    "\n",
    "        # Create new dataframe with features\n",
    "        result_df = pd.DataFrame(feature_dict)\n",
    "\n",
    "        # Fill missing values\n",
    "        numeric_columns = result_df.select_dtypes(include=[np.number]).columns\n",
    "        result_df[numeric_columns] = result_df[numeric_columns].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def predict_window(self, historical_data: pd.DataFrame) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "        \"\"\"\n",
    "        Make predictions using both models and require agreement.\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        features_df = self.prepare_time_series_features(historical_data)\n",
    "\n",
    "        # Get predictions from both models\n",
    "        xgb_predictions = self.predictor.predict(features_df, model_type='xgboost')\n",
    "        rf_predictions = self.predictor.predict(features_df, model_type='randomforest')\n",
    "\n",
    "        # Combine predictions where models agree\n",
    "        ensemble_predictions = {}\n",
    "\n",
    "        for machine_id in features_df['machine_id'].unique():\n",
    "            machine_xgb = xgb_predictions[machine_id]\n",
    "            machine_rf = rf_predictions[machine_id]\n",
    "\n",
    "            ensemble_predictions[machine_id] = {\n",
    "                'agreed_alerts': {},\n",
    "                'probabilities': {\n",
    "                    'xgboost': machine_xgb,\n",
    "                    'randomforest': machine_rf\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Check for agreement between models\n",
    "            for alert_type in self.predictor.alert_types:\n",
    "                xgb_prob = machine_xgb[alert_type]\n",
    "                rf_prob = machine_rf[alert_type]\n",
    "\n",
    "                # Models agree if both predict above threshold\n",
    "                if (xgb_prob > self.agreement_threshold and\n",
    "                        rf_prob > self.agreement_threshold):\n",
    "                    ensemble_predictions[machine_id]['agreed_alerts'][alert_type] = {\n",
    "                        'probability': (xgb_prob + rf_prob) / 2,\n",
    "                        'confidence': min(xgb_prob, rf_prob) / max(xgb_prob, rf_prob)\n",
    "                    }\n",
    "\n",
    "        return ensemble_predictions\n",
    "\n",
    "    def visualize_predictions(self, historical_data: pd.DataFrame,\n",
    "                              predictions: Dict[str, Dict[str, Dict[str, float]]],\n",
    "                              machine_id: str = None):\n",
    "        \"\"\"\n",
    "        Visualize the predictions alongside historical data.\n",
    "        \"\"\"\n",
    "        if machine_id is None:\n",
    "            machine_id = list(predictions.keys())[0]\n",
    "\n",
    "        machine_data = historical_data[historical_data['machine_id'] == machine_id].copy()\n",
    "        machine_data['Time'] = pd.to_datetime(machine_data['Time'])\n",
    "\n",
    "        # Create the visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[2, 1])\n",
    "\n",
    "        # Plot historical pressure data\n",
    "        ax1.plot(machine_data['Time'], machine_data['ChlPrs'],\n",
    "                 label='Pressure', color='blue', alpha=0.6)\n",
    "        ax1.set_title(f'Historical Data and Predictions for {machine_id}')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('Pressure')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Add rolling mean and std to the plot\n",
    "        features = self.prepare_time_series_features(machine_data)\n",
    "        ax1.plot(features['Time'], features['rolling_mean'],\n",
    "                 label='Rolling Mean (24h)', color='green', alpha=0.5)\n",
    "        ax1.fill_between(features['Time'],\n",
    "                         features['rolling_mean'] - features['rolling_std'],\n",
    "                         features['rolling_mean'] + features['rolling_std'],\n",
    "                         color='green', alpha=0.2, label='±1 Std Dev')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot model probabilities\n",
    "        machine_preds = predictions[machine_id]\n",
    "        last_time = machine_data['Time'].max()\n",
    "        prediction_times = [last_time + timedelta(hours=i)\n",
    "                            for i in range(self.prediction_window)]\n",
    "\n",
    "        for alert_type in self.predictor.alert_types:\n",
    "            xgb_prob = machine_preds['probabilities']['xgboost'][alert_type]\n",
    "            rf_prob = machine_preds['probabilities']['randomforest'][alert_type]\n",
    "\n",
    "            ax2.plot([prediction_times[0], prediction_times[-1]],\n",
    "                     [xgb_prob, xgb_prob],\n",
    "                     '--', label=f'XGB {alert_type}')\n",
    "            ax2.plot([prediction_times[0], prediction_times[-1]],\n",
    "                     [rf_prob, rf_prob],\n",
    "                     ':', label=f'RF {alert_type}')\n",
    "\n",
    "            # Highlight agreed alerts\n",
    "            if alert_type in machine_preds['agreed_alerts']:\n",
    "                alert_info = machine_preds['agreed_alerts'][alert_type]\n",
    "                ax2.axhspan(self.agreement_threshold,\n",
    "                            alert_info['probability'],\n",
    "                            alpha=0.3,\n",
    "                            color='red',\n",
    "                            label=f'Agreed {alert_type} Alert')\n",
    "\n",
    "        ax2.axhline(y=self.agreement_threshold, color='r', linestyle='-',\n",
    "                    alpha=0.3, label='Agreement Threshold')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Alert Probability')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def simulate_production_monitoring(predictor: EnsembleAlertPredictor,\n",
    "                                  data: pd.DataFrame,\n",
    "                                  start_date: str,\n",
    "                                  end_date: str,\n",
    "                                  step_hours: int = 24):\n",
    "    \"\"\"\n",
    "    Simulate production monitoring by stepping through time windows.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "    current_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        # Get historical window\n",
    "        historical_window = data[\n",
    "            (data['Time'] >= current_date - timedelta(hours=predictor.lookback_window)) &\n",
    "            (data['Time'] <= current_date)\n",
    "        ].copy()\n",
    "\n",
    "        if not historical_window.empty:\n",
    "            # Make prediction\n",
    "            predictions = predictor.predict_window(historical_window)\n",
    "\n",
    "            # Store results\n",
    "            for machine_id, machine_preds in predictions.items():\n",
    "                if machine_preds['agreed_alerts']:\n",
    "                    results.append({\n",
    "                        'time': current_date,\n",
    "                        'machine_id': machine_id,\n",
    "                        'predictions': machine_preds\n",
    "                    })\n",
    "\n",
    "            # Visualize some predictions\n",
    "            if len(results) > 0 and len(results) % 5 == 0:  # Visualize every 5th prediction\n",
    "                predictor.visualize_predictions(historical_window, predictions)\n",
    "\n",
    "        current_date += timedelta(hours=step_hours)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ... (rest of the code remains the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_42071/487203991.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  window=24, min_periods=1).mean().transform(lambda x: x.fillna(method='bfill')).values\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_42071/487203991.py:43: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  window=24, min_periods=1).std().transform(lambda x: x.fillna(method='bfill')).values\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m historical_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Simulate production monitoring\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_production_monitoring\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistorical_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2025-01-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[23], line 208\u001b[0m, in \u001b[0;36msimulate_production_monitoring\u001b[0;34m(predictor, data, start_date, end_date, step_hours)\u001b[0m\n\u001b[1;32m    201\u001b[0m historical_window \u001b[38;5;241m=\u001b[39m data[\n\u001b[1;32m    202\u001b[0m     (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_date \u001b[38;5;241m-\u001b[39m timedelta(hours\u001b[38;5;241m=\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mlookback_window)) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m    203\u001b[0m     (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m current_date)\n\u001b[1;32m    204\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m historical_window\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorical_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m machine_id, machine_preds \u001b[38;5;129;01min\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 77\u001b[0m, in \u001b[0;36mEnsembleAlertPredictor.predict_window\u001b[0;34m(self, historical_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mMake predictions using both models and require agreement.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m features_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_time_series_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorical_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Get predictions from both models\u001b[39;00m\n\u001b[1;32m     80\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict(features_df, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m, in \u001b[0;36mEnsembleAlertPredictor.prepare_time_series_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     61\u001b[0m         feature_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m168.0\u001b[39m  \u001b[38;5;66;03m# One week in hours\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Create new dataframe with features\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Fill missing values\u001b[39;00m\n\u001b[1;32m     67\u001b[0m numeric_columns \u001b[38;5;241m=\u001b[39m result_df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "output_dir = \"production_models_solo\"\n",
    "\n",
    "# Train models (if not already trained and saved)\n",
    "# xgb_predictor = AlertPredictor(model_type='xgboost')\n",
    "# xgb_predictor.train(folder, prediction_window)\n",
    "\n",
    "# rf_predictor = AlertPredictor(model_type='randomforest')\n",
    "# rf_predictor.train(folder, prediction_window)\n",
    "\n",
    "# Save the trained models (if not already saved)\n",
    "# save_trained_models(xgb_predictor, rf_predictor, output_dir)\n",
    "\n",
    "# Load the production predictor\n",
    "predictor = load_production_predictor(output_dir)\n",
    "\n",
    "# Create the ensemble predictor\n",
    "ensemble_predictor = EnsembleAlertPredictor(\n",
    "    base_predictor=predictor,\n",
    "    lookback_window=168,  # 7 days of historical data\n",
    "    prediction_window=168,  # 7 days prediction window\n",
    "    agreement_threshold=0.7\n",
    ")\n",
    "\n",
    "# Load some historical data for testing\n",
    "data_files = [f for f in os.listdir(folder) if f.endswith('_alerts.csv')]\n",
    "dfs = []\n",
    "for file in data_files:\n",
    "    df = pd.read_csv(os.path.join(folder, file))\n",
    "    machine_id = file.split('_')[0]\n",
    "    df['machine_id'] = machine_id\n",
    "    dfs.append(df)\n",
    "\n",
    "historical_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Simulate production monitoring\n",
    "results = simulate_production_monitoring(\n",
    "    predictor=ensemble_predictor,\n",
    "    data=historical_data,\n",
    "    start_date='2023-01-01',\n",
    "    end_date='2025-01-31',\n",
    "    step_hours=24\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"\\nPredictions for {result['machine_id']} at {result['time']}:\")\n",
    "    if result['predictions']['agreed_alerts']:\n",
    "        for alert_type, alert_info in result['predictions']['agreed_alerts'].items():\n",
    "            print(f\"{alert_type} Alert: Probability = {alert_info['probability']:.3f}, \"\n",
    "                  f\"Confidence = {alert_info['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(\"No agreed alerts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
