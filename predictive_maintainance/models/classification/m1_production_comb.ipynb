{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlertPredictor:\n",
    "    def __init__(self, model_type='xgboost'):\n",
    "        \"\"\"\n",
    "        Initializes the AlertPredictor with the specified model type ('xgboost' or 'randomforest').\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH', ] # 'SIGMA']\n",
    "        self.features = ['ChlPrs',\n",
    "                        #  'hour',\n",
    "                        #  'day_of_week',\n",
    "                        #  'month',\n",
    "                        #  'is_weekend',\n",
    "                         'rolling_mean', 'rolling_std'] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses data from CSV files in the specified folder.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for i in range(9, 16):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            df = pd.read_csv(os.path.join(folder, file_name))\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineers features from the preprocessed data.\n",
    "        \"\"\"\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Calculate time since last alert for each type\n",
    "        for alert_type in self.alert_types:\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_for_classification(self, df, target_alert_type, prediction_window):\n",
    "        \"\"\"\n",
    "        Prepares the data for training the classification model.\n",
    "        \"\"\"\n",
    "        df['target'] = df.groupby('machine_id').apply(\n",
    "            lambda x: (x['ALERT'] == target_alert_type).rolling(window=prediction_window).max().shift(-prediction_window + 1)).reset_index(level=0,\n",
    "                                                                                                                                           drop=True)\n",
    "\n",
    "        X = df[self.features]\n",
    "        y = df['target'].fillna(0)  # Fill NaN with 0 (no alert)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Trains and evaluates the classification model.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            # XGBoost configuration for imbalanced classification\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  # Handle class imbalance\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                early_stopping_rounds=10,\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=0)\n",
    "        elif self.model_type == 'randomforest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'xgboost' or 'randomforest'.\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder, prediction_window=7):\n",
    "        \"\"\"\n",
    "        Trains the models for each alert type.\n",
    "        \"\"\"\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\nTraining model for {alert_type} alerts:\")\n",
    "            X, y = self.prepare_data_for_classification(df, alert_type, prediction_window)\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Makes predictions on new data.\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        for alert_type in self.alert_types:\n",
    "            X_new = new_data[self.features]\n",
    "            X_new_scaled = self.scalers[alert_type].transform(X_new)\n",
    "            alert_probability = self.models[alert_type].predict_proba(X_new_scaled)[0, 1]\n",
    "            predictions[alert_type] = alert_probability\n",
    "        return predictions\n",
    "\n",
    "    def visualize_alerts(self, df, target_alert_type, prediction_window, probability_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Visualizes actual alerts and high-risk periods.\n",
    "        \"\"\"\n",
    "        X = df[self.features]\n",
    "        X_scaled = self.scalers[target_alert_type].transform(X)\n",
    "\n",
    "        df['alert_probability'] = self.models[target_alert_type].predict_proba(X_scaled)[:, 1]\n",
    "        df['high_risk'] = df['alert_probability'] > probability_threshold\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        machines = df['machine_id'].unique()\n",
    "        n_machines = len(machines)\n",
    "\n",
    "        for i, machine_id in enumerate(machines):\n",
    "            machine_df = df[df['machine_id'] == machine_id]\n",
    "\n",
    "            # Plot actual alerts\n",
    "            alerts = machine_df[machine_df['ALERT'] == target_alert_type]\n",
    "            plt.scatter(alerts['Time'], [i - 0.2] * len(alerts), marker='o', s=100,\n",
    "                        label=f'Actual {target_alert_type} Alert' if i == 0 else \"\")\n",
    "\n",
    "            # Plot high-risk periods\n",
    "            high_risk_periods = machine_df[machine_df['high_risk']]\n",
    "            plt.scatter(high_risk_periods['Time'], [i + 0.2] * len(high_risk_periods), marker='x', s=100,\n",
    "                        label=f'High Risk Period ({target_alert_type})' if i == 0 else \"\")\n",
    "\n",
    "            plt.text(df['Time'].min(), i, machine_id, va='center', ha='right', fontweight='bold')\n",
    "\n",
    "        plt.yticks(range(n_machines), machines)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Machine ID')\n",
    "        plt.title(f'Actual Alerts vs High Risk Periods for {target_alert_type} Alerts')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAlertPredictor:\n",
    "    def __init__(self, model_types=['xgboost', 'randomforest']):\n",
    "        \"\"\"\n",
    "        Initializes the ProductionAlertPredictor with specified model types.\n",
    "\n",
    "        Args:\n",
    "            model_types (list): List of model types to use ('xgboost' and/or 'randomforest')\n",
    "        \"\"\"\n",
    "        self.model_types = model_types\n",
    "        self.models = {model_type: {} for model_type in model_types}\n",
    "        self.scalers = {model_type: {} for model_type in model_types}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.features = [\n",
    "            'ChlPrs',\n",
    "            # 'hour',\n",
    "            # 'day_of_week',\n",
    "            # 'month',\n",
    "            # 'is_weekend',\n",
    "            'rolling_mean',\n",
    "            'rolling_std'] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineers features from the input DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(df['Time'].iloc[0], str):\n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Calculate time since last alert for each type\n",
    "        for alert_type in self.alert_types:\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "            ).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "\n",
    "        return df[self.features]\n",
    "\n",
    "    def save_models(self, output_dir):\n",
    "        \"\"\"\n",
    "        Saves trained models, scalers, and metadata to disk.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the models and related files\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model_types': self.model_types,\n",
    "            'alert_types': self.alert_types,\n",
    "            'features': self.features,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "        # Save models and scalers\n",
    "        for model_type in self.model_types:\n",
    "            model_dir = os.path.join(output_dir, model_type)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                # Save model\n",
    "                model_path = os.path.join(model_dir, f'{alert_type}_model.pkl')\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(self.models[model_type][alert_type], f)\n",
    "\n",
    "                # Save scaler\n",
    "                scaler_path = os.path.join(model_dir, f'{alert_type}_scaler.pkl')\n",
    "                with open(scaler_path, 'wb') as f:\n",
    "                    pickle.dump(self.scalers[model_type][alert_type], f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_models(cls, model_dir):\n",
    "        \"\"\"\n",
    "        Loads trained models, scalers, and metadata from disk.\n",
    "\n",
    "        Args:\n",
    "            model_dir (str): Directory containing the saved models and related files\n",
    "\n",
    "        Returns:\n",
    "            ProductionAlertPredictor: Initialized instance with loaded models\n",
    "        \"\"\"\n",
    "        # Load metadata\n",
    "        with open(os.path.join(model_dir, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Initialize predictor with metadata\n",
    "        predictor = cls(model_types=metadata['model_types'])\n",
    "        predictor.alert_types = metadata['alert_types']\n",
    "        predictor.features = metadata['features']\n",
    "\n",
    "        # Load models and scalers\n",
    "        for model_type in predictor.model_types:\n",
    "            model_type_dir = os.path.join(model_dir, model_type)\n",
    "\n",
    "            for alert_type in predictor.alert_types:\n",
    "                # Load model\n",
    "                model_path = os.path.join(model_type_dir, f'{alert_type}_model.pkl')\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    predictor.models[model_type][alert_type] = pickle.load(f)\n",
    "\n",
    "                # Load scaler\n",
    "                scaler_path = os.path.join(model_type_dir, f'{alert_type}_scaler.pkl')\n",
    "                with open(scaler_path, 'rb') as f:\n",
    "                    predictor.scalers[model_type][alert_type] = pickle.load(f)\n",
    "\n",
    "        return predictor\n",
    "\n",
    "    def predict(self, new_data, threshold=0.7, ensemble_method='unanimous'):\n",
    "        \"\"\"\n",
    "        Makes predictions using the ensemble of models.\n",
    "\n",
    "        Args:\n",
    "            new_data (pd.DataFrame): DataFrame containing new data to predict on\n",
    "            threshold (float): Probability threshold for positive prediction\n",
    "            ensemble_method (str): How to combine predictions ('unanimous' or 'majority')\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing predictions and probabilities for each alert type\n",
    "        \"\"\"\n",
    "        if not isinstance(new_data, pd.DataFrame):\n",
    "            raise ValueError(\"new_data must be a pandas DataFrame\")\n",
    "\n",
    "        # Engineer features for the new data\n",
    "        X_new = self.engineer_features(new_data)\n",
    "\n",
    "        results = {}\n",
    "        for alert_type in self.alert_types:\n",
    "            model_predictions = []\n",
    "            model_probabilities = []\n",
    "\n",
    "            # Get predictions from each model\n",
    "            for model_type in self.model_types:\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(X_new)\n",
    "                probabilities = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "                predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "                model_predictions.append(predictions)\n",
    "                model_probabilities.append(probabilities)\n",
    "\n",
    "            # Combine predictions based on ensemble method\n",
    "            if ensemble_method == 'unanimous':\n",
    "                final_predictions = np.all(model_predictions, axis=0)\n",
    "            elif ensemble_method == 'majority':\n",
    "                final_predictions = np.mean(model_predictions, axis=0) >= 0.5\n",
    "            else:\n",
    "                raise ValueError(\"ensemble_method must be 'unanimous' or 'majority'\")\n",
    "\n",
    "            # Average probabilities across models\n",
    "            avg_probabilities = np.mean(model_probabilities, axis=0)\n",
    "\n",
    "            results[alert_type] = {\n",
    "                'prediction': final_predictions[0],\n",
    "                'probability': avg_probabilities[0],\n",
    "                'model_probabilities': {\n",
    "                    model_type: probs[0]\n",
    "                    for model_type, probs in zip(self.model_types, model_probabilities)\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "def train_production_models(data_folder, output_dir, prediction_window=7):\n",
    "    \"\"\"\n",
    "    Trains and saves production models.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Folder containing training data\n",
    "        output_dir (str): Directory to save trained models\n",
    "        prediction_window (int): Prediction window in days\n",
    "    \"\"\"\n",
    "    # Initialize original predictor instances\n",
    "    xgb_predictor = AlertPredictor(model_type='xgboost')\n",
    "    rf_predictor = AlertPredictor(model_type='randomforest')\n",
    "\n",
    "    # Train both models\n",
    "    xgb_predictor.train(data_folder, prediction_window)\n",
    "    rf_predictor.train(data_folder, prediction_window)\n",
    "\n",
    "    # Initialize production predictor\n",
    "    prod_predictor = ProductionAlertPredictor(['xgboost', 'randomforest'])\n",
    "\n",
    "    # Transfer trained models and scalers\n",
    "    for alert_type in prod_predictor.alert_types:\n",
    "        prod_predictor.models['xgboost'][alert_type] = xgb_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['xgboost'][alert_type] = xgb_predictor.scalers[alert_type]\n",
    "\n",
    "        prod_predictor.models['randomforest'][alert_type] = rf_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['randomforest'][alert_type] = rf_predictor.scalers[alert_type]\n",
    "\n",
    "    # Save the production models\n",
    "    prod_predictor.save_models(output_dir)\n",
    "    return prod_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for LOW alerts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/1073905821.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['target'] = df.groupby('machine_id').apply(\n"
     ]
    }
   ],
   "source": [
    "# Train and save the models\n",
    "output_dir = \"production_models\"\n",
    "folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "prediction_window = 7  # days\n",
    "\n",
    "predictor = train_production_models(\n",
    "    data_folder=folder,\n",
    "    output_dir=output_dir,\n",
    "    prediction_window=prediction_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/2830613950.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/2830613950.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39783/2830613950.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- day_of_week\n- hour\n- is_weekend\n- month\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-01 00:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTOL-09\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChlPrs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1.5\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALERT\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get predictions with unanimous agreement (both models must agree)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munanimous\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Print predictions\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alert_type, result \u001b[38;5;129;01min\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[8], line 145\u001b[0m, in \u001b[0;36mProductionAlertPredictor.predict\u001b[0;34m(self, new_data, threshold, ensemble_method)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Get predictions from each model\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_types:\n\u001b[0;32m--> 145\u001b[0m     X_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43malert_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model_type][alert_type]\u001b[38;5;241m.\u001b[39mpredict_proba(X_scaled)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    147\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m (probabilities \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- day_of_week\n- hour\n- is_weekend\n- month\n"
     ]
    }
   ],
   "source": [
    "# Load the saved models\n",
    "predictor = ProductionAlertPredictor.load_models(\"production_models\")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'Time': ['2024-01-01 00:00:00'],\n",
    "    'machine_id': ['HTOL-09'],\n",
    "    'ChlPrs': [1.5],\n",
    "    'ALERT': ['NONE']\n",
    "})\n",
    "\n",
    "# Get predictions with unanimous agreement (both models must agree)\n",
    "predictions = predictor.predict(\n",
    "    new_data,\n",
    "    threshold=0.7,\n",
    "    ensemble_method='unanimous'\n",
    ")\n",
    "\n",
    "# Print predictions\n",
    "for alert_type, result in predictions.items():\n",
    "    print(f\"\\n{alert_type} Alert:\")\n",
    "    print(f\"Prediction: {'Yes' if result['prediction'] else 'No'}\")\n",
    "    print(f\"Average Probability: {result['probability']:.3f}\")\n",
    "    print(\"Individual Model Probabilities:\")\n",
    "    for model, prob in result['model_probabilities'].items():\n",
    "        print(f\"  - {model}: {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
