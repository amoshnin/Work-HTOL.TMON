{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAlertPredictor:\n",
    "    def __init__(self, model_type='xgboost', sequence_length=24):\n",
    "        print(f\"\\nInitializing {model_type.upper()} predictor with sequence length {sequence_length}\")\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.base_features = ['ChlPrs', 'rolling_mean', 'rolling_std']\n",
    "        self.features = []\n",
    "        for feature in self.base_features:\n",
    "            self.features.extend([f'{feature}_t{i}' for i in range(sequence_length)])\n",
    "        self.features.extend([f'time_since_{at}' for at in self.alert_types])\n",
    "        print(f\"Total features initialized: {len(self.features)}\")\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        print(\"\\nLoading and preprocessing data:\")\n",
    "        dfs = []\n",
    "        total_files = 7  # files from HTOL-09 to HTOL-15\n",
    "\n",
    "        for i in tqdm(range(9, 16), desc=\"Loading files\"):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            print(f\"\\nProcessing {file_name}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"- Loaded {len(df)} rows from {file_name}\")\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        print(\"\\nCombining datasets...\")\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        print(f\"Total combined dataset size: {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "\n",
    "    def create_sequences(self, df):\n",
    "        print(\"\\nCreating sequences...\")\n",
    "        sequences = []\n",
    "        labels = []\n",
    "\n",
    "        total_machines = len(df['machine_id'].unique())\n",
    "        for idx, machine_id in enumerate(df['machine_id'].unique(), 1):\n",
    "            print(f\"\\nProcessing machine {machine_id} ({idx}/{total_machines})\")\n",
    "            machine_data = df[df['machine_id'] == machine_id].copy()\n",
    "\n",
    "            sequence_count = len(machine_data) - self.sequence_length\n",
    "            print(f\"- Creating {sequence_count} sequences for {machine_id}\")\n",
    "\n",
    "            for i in tqdm(range(sequence_count), desc=\"Creating sequences\"):\n",
    "                sequence = machine_data.iloc[i:i + self.sequence_length]\n",
    "                target_row = machine_data.iloc[i + self.sequence_length]\n",
    "\n",
    "                sequence_features = {}\n",
    "                for feature in self.base_features:\n",
    "                    for j in range(self.sequence_length):\n",
    "                        sequence_features[f'{feature}_t{j}'] = sequence.iloc[j][feature]\n",
    "\n",
    "                for alert_type in self.alert_types:\n",
    "                    sequence_features[f'time_since_{alert_type}'] = target_row[f'time_since_{alert_type}']\n",
    "\n",
    "                sequences.append(sequence_features)\n",
    "                labels.append(1 if target_row['ALERT'] == self.current_alert_type else 0)\n",
    "\n",
    "        print(f\"\\nTotal sequences created: {len(sequences)}\")\n",
    "        print(f\"Positive samples: {sum(labels)}\")\n",
    "        print(f\"Negative samples: {len(labels) - sum(labels)}\")\n",
    "        return pd.DataFrame(sequences), np.array(labels)\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        print(\"\\nEngineering features...\")\n",
    "\n",
    "        print(\"- Calculating rolling statistics\")\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        print(\"- Computing time since last alerts\")\n",
    "        for alert_type in tqdm(self.alert_types, desc=\"Processing alert types\"):\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "            ).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "\n",
    "        print(\"Feature engineering complete\")\n",
    "        return df\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        print(\"\\nTraining and evaluating classifier:\")\n",
    "        print(f\"- Total samples: {len(X)}\")\n",
    "        print(f\"- Training set size: {int(len(X) * (1-test_size))}\")\n",
    "        print(f\"- Test set size: {int(len(X) * test_size)}\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        print(\"Scaling features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            print(\"\\nTraining XGBoost classifier...\")\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nTraining Random Forest classifier...\")\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        print(\"Fitting model...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        print(\"\\nEvaluating model performance:\")\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting training process for {self.model_type.upper()} model\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        print(\"\\nStep 1: Loading and preprocessing data\")\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "\n",
    "        print(\"\\nStep 2: Engineering features\")\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training model for {alert_type} alerts\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            self.current_alert_type = alert_type\n",
    "            print(\"\\nStep 3: Creating sequences\")\n",
    "            X, y = self.create_sequences(df)\n",
    "\n",
    "            print(\"\\nStep 4: Training and evaluating model\")\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "            print(f\"\\nCompleted training for {alert_type} alerts\")\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training complete for all alert types\")\n",
    "        print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionSequencePredictor:\n",
    "    def __init__(self, model_types=['xgboost', 'randomforest'], sequence_length=24):\n",
    "        \"\"\"\n",
    "        Initializes the ProductionSequencePredictor with multiple model types.\n",
    "        \"\"\"\n",
    "        self.model_types = model_types\n",
    "        self.sequence_length = sequence_length\n",
    "        self.models = {model_type: {} for model_type in model_types}\n",
    "        self.scalers = {model_type: {} for model_type in model_types}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.base_features = ['ChlPrs', 'rolling_mean', 'rolling_std']\n",
    "        self.features = []\n",
    "        for feature in self.base_features:\n",
    "            self.features.extend([f'{feature}_t{i}' for i in range(sequence_length)])\n",
    "        self.features.extend([f'time_since_{at}' for at in self.alert_types])\n",
    "\n",
    "    def prepare_sequence(self, df):\n",
    "        \"\"\"\n",
    "        Prepares a sequence of data points for prediction.\n",
    "        \"\"\"\n",
    "        if len(df) < self.sequence_length:\n",
    "            raise ValueError(f\"Input data must contain at least {self.sequence_length} points\")\n",
    "\n",
    "        sequence = df.iloc[-self.sequence_length:]\n",
    "        sequence_features = {}\n",
    "\n",
    "        for feature in self.base_features:\n",
    "            for j in range(self.sequence_length):\n",
    "                sequence_features[f'{feature}_t{j}'] = sequence.iloc[j][feature]\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            sequence_features[f'time_since_{alert_type}'] = df.iloc[-1][f'time_since_{alert_type}']\n",
    "\n",
    "        return pd.DataFrame([sequence_features])\n",
    "\n",
    "    def save_models(self, output_dir):\n",
    "        \"\"\"\n",
    "        Saves trained models and metadata to disk.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        metadata = {\n",
    "            'model_types': self.model_types,\n",
    "            'alert_types': self.alert_types,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'features': self.features,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "        for model_type in self.model_types:\n",
    "            model_dir = os.path.join(output_dir, model_type)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                with open(os.path.join(model_dir, f'{alert_type}_model.pkl'), 'wb') as f:\n",
    "                    pickle.dump(self.models[model_type][alert_type], f)\n",
    "                with open(os.path.join(model_dir, f'{alert_type}_scaler.pkl'), 'wb') as f:\n",
    "                    pickle.dump(self.scalers[model_type][alert_type], f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_models(cls, model_dir):\n",
    "        \"\"\"\n",
    "        Loads trained models from disk.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(model_dir, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        predictor = cls(\n",
    "            model_types=metadata['model_types'],\n",
    "            sequence_length=metadata['sequence_length']\n",
    "        )\n",
    "\n",
    "        for model_type in predictor.model_types:\n",
    "            model_type_dir = os.path.join(model_dir, model_type)\n",
    "\n",
    "            for alert_type in predictor.alert_types:\n",
    "                with open(os.path.join(model_type_dir, f'{alert_type}_model.pkl'), 'rb') as f:\n",
    "                    predictor.models[model_type][alert_type] = pickle.load(f)\n",
    "                with open(os.path.join(model_type_dir, f'{alert_type}_scaler.pkl'), 'rb') as f:\n",
    "                    predictor.scalers[model_type][alert_type] = pickle.load(f)\n",
    "\n",
    "        return predictor\n",
    "\n",
    "    def predict(self, df, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Makes predictions using the ensemble of models.\n",
    "        \"\"\"\n",
    "        sequence = self.prepare_sequence(df)\n",
    "        results = {}\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            model_predictions = []\n",
    "            model_probabilities = []\n",
    "\n",
    "            for model_type in self.model_types:\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(sequence)\n",
    "                probabilities = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "                predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "                model_predictions.append(predictions)\n",
    "                model_probabilities.append(probabilities)\n",
    "\n",
    "            # Unanimous ensemble prediction\n",
    "            final_predictions = np.all(model_predictions, axis=0)\n",
    "            avg_probabilities = np.mean(model_probabilities, axis=0)\n",
    "\n",
    "            results[alert_type] = {\n",
    "                'prediction': final_predictions[0],\n",
    "                'probability': avg_probabilities[0],\n",
    "                'model_probabilities': {\n",
    "                    model_type: probs[0]\n",
    "                    for model_type, probs in zip(self.model_types, model_probabilities)\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_production_models(data_folder, output_dir, sequence_length=24):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING PRODUCTION MODELS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"- Data folder: {data_folder}\")\n",
    "    print(f\"- Output directory: {output_dir}\")\n",
    "    print(f\"- Sequence length: {sequence_length}\")\n",
    "\n",
    "    print(\"\\nStep 1: Training XGBoost models\")\n",
    "    xgb_predictor = SequenceAlertPredictor(model_type='xgboost', sequence_length=sequence_length)\n",
    "    xgb_predictor.train(data_folder)\n",
    "\n",
    "    print(\"\\nStep 2: Training Random Forest models\")\n",
    "    rf_predictor = SequenceAlertPredictor(model_type='randomforest', sequence_length=sequence_length)\n",
    "    rf_predictor.train(data_folder)\n",
    "\n",
    "    print(\"\\nStep 3: Initializing production predictor\")\n",
    "    prod_predictor = ProductionSequencePredictor(['xgboost', 'randomforest'], sequence_length)\n",
    "\n",
    "    print(\"\\nStep 4: Combining models\")\n",
    "    for alert_type in tqdm(prod_predictor.alert_types, desc=\"Combining models\"):\n",
    "        prod_predictor.models['xgboost'][alert_type] = xgb_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['xgboost'][alert_type] = xgb_predictor.scalers[alert_type]\n",
    "        prod_predictor.models['randomforest'][alert_type] = rf_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['randomforest'][alert_type] = rf_predictor.scalers[alert_type]\n",
    "\n",
    "    print(\"\\nStep 5: Saving models\")\n",
    "    prod_predictor.save_models(output_dir)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    return prod_predictor\n",
    "\n",
    "def visualize_predictions(predictor, df, alert_type, window_size=168):  # 1 week\n",
    "    \"\"\"\n",
    "    Visualizes predictions vs actual alerts.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual_alerts = []\n",
    "    timestamps = []\n",
    "\n",
    "    for i in range(predictor.sequence_length, len(df)):\n",
    "        sequence_df = df.iloc[max(0, i-window_size):i]\n",
    "        if len(sequence_df) >= predictor.sequence_length:\n",
    "            pred = predictor.predict(sequence_df)\n",
    "            predictions.append(pred[alert_type]['probability'])\n",
    "            actual_alerts.append(1 if df.iloc[i]['ALERT'] == alert_type else 0)\n",
    "            timestamps.append(df.iloc[i]['Time'])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(timestamps, predictions, label='Prediction Probability', color='blue', alpha=0.6)\n",
    "    plt.scatter([t for t, a in zip(timestamps, actual_alerts) if a == 1],\n",
    "                [1 for a in actual_alerts if a == 1],\n",
    "                color='red', label='Actual Alerts', marker='x', s=100)\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.3, label='Threshold (0.7)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Alert Probability')\n",
    "    plt.title(f'Predicted vs Actual {alert_type} Alerts')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_production_models(data_folder, output_dir, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Trains and saves production models.\n",
    "    \"\"\"\n",
    "    print(\"Initiating XGB\")\n",
    "    xgb_predictor = SequenceAlertPredictor(model_type='xgboost', sequence_length=sequence_length)\n",
    "    print(\"Initiating RF\")\n",
    "    rf_predictor = SequenceAlertPredictor(model_type='randomforest', sequence_length=sequence_length)\n",
    "\n",
    "    print(\"Training XGB\")\n",
    "    xgb_predictor.train(data_folder)\n",
    "    print(\"Training RF\")\n",
    "    rf_predictor.train(data_folder)\n",
    "\n",
    "    prod_predictor = ProductionSequencePredictor(['xgboost', 'randomforest'], sequence_length)\n",
    "\n",
    "    for alert_type in prod_predictor.alert_types:\n",
    "        prod_predictor.models['xgboost'][alert_type] = xgb_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['xgboost'][alert_type] = xgb_predictor.scalers[alert_type]\n",
    "        prod_predictor.models['randomforest'][alert_type] = rf_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['randomforest'][alert_type] = rf_predictor.scalers[alert_type]\n",
    "\n",
    "    prod_predictor.save_models(output_dir)\n",
    "    return prod_predictor\n",
    "\n",
    "def visualize_predictions(predictor, df, alert_type, window_size=168):  # 1 week\n",
    "    \"\"\"\n",
    "    Visualizes predictions vs actual alerts.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual_alerts = []\n",
    "    timestamps = []\n",
    "\n",
    "    for i in range(predictor.sequence_length, len(df)):\n",
    "        sequence_df = df.iloc[max(0, i-window_size):i]\n",
    "        if len(sequence_df) >= predictor.sequence_length:\n",
    "            pred = predictor.predict(sequence_df)\n",
    "            predictions.append(pred[alert_type]['probability'])\n",
    "            actual_alerts.append(1 if df.iloc[i]['ALERT'] == alert_type else 0)\n",
    "            timestamps.append(df.iloc[i]['Time'])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(timestamps, predictions, label='Prediction Probability', color='blue', alpha=0.6)\n",
    "    plt.scatter([t for t, a in zip(timestamps, actual_alerts) if a == 1],\n",
    "                [1 for a in actual_alerts if a == 1],\n",
    "                color='red', label='Actual Alerts', marker='x', s=100)\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.3, label='Threshold (0.7)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Alert Probability')\n",
    "    plt.title(f'Predicted vs Actual {alert_type} Alerts')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating XGB\n",
      "\n",
      "Initializing XGBOOST predictor with sequence length 24\n",
      "Total features initialized: 75\n",
      "Initiating RF\n",
      "\n",
      "Initializing RANDOMFOREST predictor with sequence length 24\n",
      "Total features initialized: 75\n",
      "Training XGB\n",
      "\n",
      "==================================================\n",
      "Starting training process for XGBOOST model\n",
      "==================================================\n",
      "\n",
      "Step 1: Loading and preprocessing data\n",
      "\n",
      "Loading and preprocessing data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HTOL-09_alerts.csv...\n",
      "- Loaded 100165 rows from HTOL-09_alerts.csv\n",
      "\n",
      "Processing HTOL-10_alerts.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  57%|█████▋    | 4/7 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded 268208 rows from HTOL-10_alerts.csv\n",
      "\n",
      "Processing HTOL-11_alerts.csv...\n",
      "- Loaded 103927 rows from HTOL-11_alerts.csv\n",
      "\n",
      "Processing HTOL-12_alerts.csv...\n",
      "- Loaded 93045 rows from HTOL-12_alerts.csv\n",
      "\n",
      "Processing HTOL-13_alerts.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  86%|████████▌ | 6/7 [00:00<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded 255802 rows from HTOL-13_alerts.csv\n",
      "\n",
      "Processing HTOL-14_alerts.csv...\n",
      "- Loaded 250107 rows from HTOL-14_alerts.csv\n",
      "\n",
      "Processing HTOL-15_alerts.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 7/7 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded 258447 rows from HTOL-15_alerts.csv\n",
      "\n",
      "Combining datasets...\n",
      "Total combined dataset size: 1329701 rows\n",
      "\n",
      "Step 2: Engineering features\n",
      "\n",
      "Engineering features...\n",
      "- Calculating rolling statistics\n",
      "- Computing time since last alerts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing alert types:   0%|          | 0/3 [00:00<?, ?it/s]/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/3097310397.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "Processing alert types:  33%|███▎      | 1/3 [00:00<00:00,  3.02it/s]/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/3097310397.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "Processing alert types:  67%|██████▋   | 2/3 [00:00<00:00,  3.04it/s]/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/3097310397.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "Processing alert types: 100%|██████████| 3/3 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete\n",
      "\n",
      "==================================================\n",
      "Training model for LOW alerts\n",
      "==================================================\n",
      "\n",
      "Step 3: Creating sequences\n",
      "\n",
      "Creating sequences...\n",
      "\n",
      "Processing machine HTOL-09 (1/7)\n",
      "- Creating 100141 sequences for HTOL-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences:   2%|▏         | 2487/100141 [00:09<06:28, 251.23it/s]"
     ]
    }
   ],
   "source": [
    "output_dir = \"production_sequence_models\"\n",
    "data_folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "sequence_length = 24  # 24 hours of data\n",
    "\n",
    "predictor = train_production_models(\n",
    "    data_folder=data_folder,\n",
    "    output_dir=output_dir,\n",
    "    sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved models and making predictions\n",
    "predictor = ProductionSequencePredictor.load_models(\"production_sequence_models\")\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"path_to_test_data.csv\")\n",
    "test_df['Time'] = pd.to_datetime(test_df['Time'])\n",
    "test_df = test_df.sort_values('Time')\n",
    "\n",
    "# Visualize predictions for each alert type\n",
    "for alert_type in predictor.alert_types:\n",
    "    visualize_predictions(predictor, test_df, alert_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
