{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = 1\n",
    "total_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAlertPredictor:\n",
    "    def __init__(self, model_type='xgboost', sequence_length=24):\n",
    "        print(f\"\\nInitializing {model_type.upper()} predictor with sequence length {sequence_length}\")\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.base_features = ['ChlPrs', 'rolling_mean', 'rolling_std']\n",
    "        self.features = []\n",
    "        for feature in self.base_features:\n",
    "            self.features.extend([f'{feature}_t{i}' for i in range(sequence_length)])\n",
    "        self.features.extend([f'time_since_{at}' for at in self.alert_types])\n",
    "        print(f\"Total features initialized: {len(self.features)}\")\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        print(\"\\nLoading and preprocessing data:\")\n",
    "        dfs = []\n",
    "\n",
    "        for i in tqdm(range(9, 9 + total_files), desc=\"Loading files\"):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            print(f\"\\nProcessing {file_name}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"- Loaded {len(df)} rows from {file_name}\")\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        print(\"\\nCombining datasets...\")\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        print(f\"Total combined dataset size: {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "\n",
    "    def create_sequences(self, df):\n",
    "        print(\"\\nCreating sequences...\")\n",
    "        sequences = []\n",
    "        labels = []\n",
    "\n",
    "        total_machines = len(df['machine_id'].unique())\n",
    "        for idx, machine_id in enumerate(df['machine_id'].unique(), 1):\n",
    "            print(f\"\\nProcessing machine {machine_id} ({idx}/{total_machines})\")\n",
    "            machine_data = df[df['machine_id'] == machine_id].copy()\n",
    "\n",
    "            sequence_count = len(machine_data) - self.sequence_length\n",
    "            print(f\"- Creating {sequence_count} sequences for {machine_id}\")\n",
    "\n",
    "            for i in tqdm(range(sequence_count), desc=\"Creating sequences\"):\n",
    "                sequence = machine_data.iloc[i:i + self.sequence_length]\n",
    "                target_row = machine_data.iloc[i + self.sequence_length]\n",
    "\n",
    "                sequence_features = {}\n",
    "                for feature in self.base_features:\n",
    "                    for j in range(self.sequence_length):\n",
    "                        sequence_features[f'{feature}_t{j}'] = sequence.iloc[j][feature]\n",
    "\n",
    "                for alert_type in self.alert_types:\n",
    "                    sequence_features[f'time_since_{alert_type}'] = target_row[f'time_since_{alert_type}']\n",
    "\n",
    "                sequences.append(sequence_features)\n",
    "                labels.append(1 if target_row['ALERT'] == self.current_alert_type else 0)\n",
    "\n",
    "        print(f\"\\nTotal sequences created: {len(sequences)}\")\n",
    "        print(f\"Positive samples: {sum(labels)}\")\n",
    "        print(f\"Negative samples: {len(labels) - sum(labels)}\")\n",
    "        return pd.DataFrame(sequences), np.array(labels)\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        print(\"\\nEngineering features...\")\n",
    "\n",
    "        print(\"- Calculating rolling statistics\")\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        print(\"- Computing time since last alerts\")\n",
    "        for alert_type in tqdm(self.alert_types, desc=\"Processing alert types\"):\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "            ).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "\n",
    "        print(\"Feature engineering complete\")\n",
    "        return df\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        print(\"\\nTraining and evaluating classifier:\")\n",
    "        print(f\"- Total samples: {len(X)}\")\n",
    "        print(f\"- Training set size: {int(len(X) * (1-test_size))}\")\n",
    "        print(f\"- Test set size: {int(len(X) * test_size)}\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        print(\"Scaling features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            print(\"\\nTraining XGBoost classifier...\")\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nTraining Random Forest classifier...\")\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        print(\"Fitting model...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        print(\"\\nEvaluating model performance:\")\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting training process for {self.model_type.upper()} model\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        print(\"\\nStep 1: Loading and preprocessing data\")\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "\n",
    "        print(\"\\nStep 2: Engineering features\")\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training model for {alert_type} alerts\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            self.current_alert_type = alert_type\n",
    "            print(\"\\nStep 3: Creating sequences\")\n",
    "            X, y = self.create_sequences(df)\n",
    "\n",
    "            print(\"\\nStep 4: Training and evaluating model\")\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "            print(f\"\\nCompleted training for {alert_type} alerts\")\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training complete for all alert types\")\n",
    "        print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductionSequencePredictor:\n",
    "    def __init__(self, model_types=['xgboost', 'randomforest'], sequence_length=24):\n",
    "        \"\"\"\n",
    "        Initializes the ProductionSequencePredictor with multiple model types.\n",
    "        \"\"\"\n",
    "        self.model_types = model_types\n",
    "        self.sequence_length = sequence_length\n",
    "        self.models = {model_type: {} for model_type in model_types}\n",
    "        self.scalers = {model_type: {} for model_type in model_types}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.base_features = ['ChlPrs', 'rolling_mean', 'rolling_std']\n",
    "        self.features = []\n",
    "        for feature in self.base_features:\n",
    "            self.features.extend([f'{feature}_t{i}' for i in range(sequence_length)])\n",
    "        self.features.extend([f'time_since_{at}' for at in self.alert_types])\n",
    "\n",
    "    def prepare_sequence(self, df):\n",
    "        \"\"\"\n",
    "        Prepares a sequence of data points for prediction.\n",
    "        \"\"\"\n",
    "        if len(df) < self.sequence_length:\n",
    "            raise ValueError(f\"Input data must contain at least {self.sequence_length} points\")\n",
    "\n",
    "        sequence = df.iloc[-self.sequence_length:]\n",
    "        sequence_features = {}\n",
    "\n",
    "        for feature in self.base_features:\n",
    "            for j in range(self.sequence_length):\n",
    "                sequence_features[f'{feature}_t{j}'] = sequence.iloc[j][feature]\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            sequence_features[f'time_since_{alert_type}'] = df.iloc[-1][f'time_since_{alert_type}']\n",
    "\n",
    "        return pd.DataFrame([sequence_features])\n",
    "\n",
    "    def save_models(self, output_dir):\n",
    "        \"\"\"\n",
    "        Saves trained models and metadata to disk.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        metadata = {\n",
    "            'model_types': self.model_types,\n",
    "            'alert_types': self.alert_types,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'features': self.features,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "        for model_type in self.model_types:\n",
    "            model_dir = os.path.join(output_dir, model_type)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                with open(os.path.join(model_dir, f'{alert_type}_model.pkl'), 'wb') as f:\n",
    "                    pickle.dump(self.models[model_type][alert_type], f)\n",
    "                with open(os.path.join(model_dir, f'{alert_type}_scaler.pkl'), 'wb') as f:\n",
    "                    pickle.dump(self.scalers[model_type][alert_type], f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_models(cls, model_dir):\n",
    "        \"\"\"\n",
    "        Loads trained models from disk.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(model_dir, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        predictor = cls(\n",
    "            model_types=metadata['model_types'],\n",
    "            sequence_length=metadata['sequence_length']\n",
    "        )\n",
    "\n",
    "        for model_type in predictor.model_types:\n",
    "            model_type_dir = os.path.join(model_dir, model_type)\n",
    "\n",
    "            for alert_type in predictor.alert_types:\n",
    "                with open(os.path.join(model_type_dir, f'{alert_type}_model.pkl'), 'rb') as f:\n",
    "                    predictor.models[model_type][alert_type] = pickle.load(f)\n",
    "                with open(os.path.join(model_type_dir, f'{alert_type}_scaler.pkl'), 'rb') as f:\n",
    "                    predictor.scalers[model_type][alert_type] = pickle.load(f)\n",
    "\n",
    "        return predictor\n",
    "\n",
    "    def predict(self, df, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Makes predictions using the ensemble of models.\n",
    "        \"\"\"\n",
    "        sequence = self.prepare_sequence(df)\n",
    "        results = {}\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            model_predictions = []\n",
    "            model_probabilities = []\n",
    "\n",
    "            for model_type in self.model_types:\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(sequence)\n",
    "                probabilities = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "                predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "                model_predictions.append(predictions)\n",
    "                model_probabilities.append(probabilities)\n",
    "\n",
    "            # Unanimous ensemble prediction\n",
    "            final_predictions = np.all(model_predictions, axis=0)\n",
    "            avg_probabilities = np.mean(model_probabilities, axis=0)\n",
    "\n",
    "            results[alert_type] = {\n",
    "                'prediction': final_predictions[0],\n",
    "                'probability': avg_probabilities[0],\n",
    "                'model_probabilities': {\n",
    "                    model_type: probs[0]\n",
    "                    for model_type, probs in zip(self.model_types, model_probabilities)\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_production_models(data_folder, output_dir, sequence_length=24):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING PRODUCTION MODELS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"- Data folder: {data_folder}\")\n",
    "    print(f\"- Output directory: {output_dir}\")\n",
    "    print(f\"- Sequence length: {sequence_length}\")\n",
    "\n",
    "    print(\"\\nStep 1: Training XGBoost models\")\n",
    "    xgb_predictor = SequenceAlertPredictor(model_type='xgboost', sequence_length=sequence_length)\n",
    "    xgb_predictor.train(data_folder)\n",
    "\n",
    "    print(\"\\nStep 2: Training Random Forest models\")\n",
    "    rf_predictor = SequenceAlertPredictor(model_type='randomforest', sequence_length=sequence_length)\n",
    "    rf_predictor.train(data_folder)\n",
    "\n",
    "    print(\"\\nStep 3: Initializing production predictor\")\n",
    "    prod_predictor = ProductionSequencePredictor(['xgboost', 'randomforest'], sequence_length)\n",
    "\n",
    "    print(\"\\nStep 4: Combining models\")\n",
    "    for alert_type in tqdm(prod_predictor.alert_types, desc=\"Combining models\"):\n",
    "        prod_predictor.models['xgboost'][alert_type] = xgb_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['xgboost'][alert_type] = xgb_predictor.scalers[alert_type]\n",
    "        prod_predictor.models['randomforest'][alert_type] = rf_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['randomforest'][alert_type] = rf_predictor.scalers[alert_type]\n",
    "\n",
    "    print(\"\\nStep 5: Saving models\")\n",
    "    prod_predictor.save_models(output_dir)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    return prod_predictor\n",
    "\n",
    "def visualize_predictions(predictor, df, alert_type, window_size=168):  # 1 week\n",
    "    \"\"\"\n",
    "    Visualizes predictions vs actual alerts.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual_alerts = []\n",
    "    timestamps = []\n",
    "\n",
    "    for i in range(predictor.sequence_length, len(df)):\n",
    "        sequence_df = df.iloc[max(0, i-window_size):i]\n",
    "        if len(sequence_df) >= predictor.sequence_length:\n",
    "            pred = predictor.predict(sequence_df)\n",
    "            predictions.append(pred[alert_type]['probability'])\n",
    "            actual_alerts.append(1 if df.iloc[i]['ALERT'] == alert_type else 0)\n",
    "            timestamps.append(df.iloc[i]['Time'])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(timestamps, predictions, label='Prediction Probability', color='blue', alpha=0.6)\n",
    "    plt.scatter([t for t, a in zip(timestamps, actual_alerts) if a == 1],\n",
    "                [1 for a in actual_alerts if a == 1],\n",
    "                color='red', label='Actual Alerts', marker='x', s=100)\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.3, label='Threshold (0.7)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Alert Probability')\n",
    "    plt.title(f'Predicted vs Actual {alert_type} Alerts')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_production_models(data_folder, output_dir, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Trains and saves production models.\n",
    "    \"\"\"\n",
    "    print(\"Initiating XGB\")\n",
    "    xgb_predictor = SequenceAlertPredictor(model_type='xgboost', sequence_length=sequence_length)\n",
    "    print(\"Initiating RF\")\n",
    "    rf_predictor = SequenceAlertPredictor(model_type='randomforest', sequence_length=sequence_length)\n",
    "\n",
    "    print(\"Training XGB\")\n",
    "    xgb_predictor.train(data_folder)\n",
    "    print(\"Training RF\")\n",
    "    rf_predictor.train(data_folder)\n",
    "\n",
    "    prod_predictor = ProductionSequencePredictor(['xgboost', 'randomforest'], sequence_length)\n",
    "\n",
    "    for alert_type in prod_predictor.alert_types:\n",
    "        prod_predictor.models['xgboost'][alert_type] = xgb_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['xgboost'][alert_type] = xgb_predictor.scalers[alert_type]\n",
    "        prod_predictor.models['randomforest'][alert_type] = rf_predictor.models[alert_type]\n",
    "        prod_predictor.scalers['randomforest'][alert_type] = rf_predictor.scalers[alert_type]\n",
    "\n",
    "    prod_predictor.save_models(output_dir)\n",
    "    return prod_predictor\n",
    "\n",
    "def visualize_predictions(predictor, df, alert_type, window_size=168):  # 1 week\n",
    "    \"\"\"\n",
    "    Visualizes predictions vs actual alerts.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual_alerts = []\n",
    "    timestamps = []\n",
    "\n",
    "    for i in range(predictor.sequence_length, len(df)):\n",
    "        sequence_df = df.iloc[max(0, i-window_size):i]\n",
    "        if len(sequence_df) >= predictor.sequence_length:\n",
    "            pred = predictor.predict(sequence_df)\n",
    "            predictions.append(pred[alert_type]['probability'])\n",
    "            actual_alerts.append(1 if df.iloc[i]['ALERT'] == alert_type else 0)\n",
    "            timestamps.append(df.iloc[i]['Time'])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(timestamps, predictions, label='Prediction Probability', color='blue', alpha=0.6)\n",
    "    plt.scatter([t for t, a in zip(timestamps, actual_alerts) if a == 1],\n",
    "                [1 for a in actual_alerts if a == 1],\n",
    "                color='red', label='Actual Alerts', marker='x', s=100)\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.3, label='Threshold (0.7)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Alert Probability')\n",
    "    plt.title(f'Predicted vs Actual {alert_type} Alerts')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating XGB\n",
      "\n",
      "Initializing XGBOOST predictor with sequence length 24\n",
      "Total features initialized: 75\n",
      "Initiating RF\n",
      "\n",
      "Initializing RANDOMFOREST predictor with sequence length 24\n",
      "Total features initialized: 75\n",
      "Training XGB\n",
      "\n",
      "==================================================\n",
      "Starting training process for XGBOOST model\n",
      "==================================================\n",
      "\n",
      "Step 1: Loading and preprocessing data\n",
      "\n",
      "Loading and preprocessing data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HTOL-09_alerts.csv...\n",
      "- Loaded 100165 rows from HTOL-09_alerts.csv\n",
      "\n",
      "Processing HTOL-10_alerts.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded 268208 rows from HTOL-10_alerts.csv\n",
      "\n",
      "Combining datasets...\n",
      "Total combined dataset size: 368373 rows\n",
      "\n",
      "Step 2: Engineering features\n",
      "\n",
      "Engineering features...\n",
      "- Calculating rolling statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Computing time since last alerts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing alert types:   0%|          | 0/3 [00:00<?, ?it/s]/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/317633651.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/317633651.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "Processing alert types:  67%|██████▋   | 2/3 [00:00<00:00, 11.02it/s]/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_43256/317633651.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "Processing alert types: 100%|██████████| 3/3 [00:00<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete\n",
      "\n",
      "==================================================\n",
      "Training model for LOW alerts\n",
      "==================================================\n",
      "\n",
      "Step 3: Creating sequences\n",
      "\n",
      "Creating sequences...\n",
      "\n",
      "Processing machine HTOL-09 (1/2)\n",
      "- Creating 100141 sequences for HTOL-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 100141/100141 [06:36<00:00, 252.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing machine HTOL-10 (2/2)\n",
      "- Creating 268184 sequences for HTOL-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 268184/268184 [17:24<00:00, 256.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sequences created: 368325\n",
      "Positive samples: 78\n",
      "Negative samples: 368247\n",
      "\n",
      "Step 4: Training and evaluating model\n",
      "\n",
      "Training and evaluating classifier:\n",
      "- Total samples: 368325\n",
      "- Training set size: 294660\n",
      "- Test set size: 73665\n",
      "Scaling features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost classifier...\n",
      "Fitting model...\n",
      "\n",
      "Evaluating model performance:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     73649\n",
      "           1       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00     73665\n",
      "   macro avg       1.00      1.00      1.00     73665\n",
      "weighted avg       1.00      1.00      1.00     73665\n",
      "\n",
      "\n",
      "Completed training for LOW alerts\n",
      "\n",
      "==================================================\n",
      "Training model for MEDIUM alerts\n",
      "==================================================\n",
      "\n",
      "Step 3: Creating sequences\n",
      "\n",
      "Creating sequences...\n",
      "\n",
      "Processing machine HTOL-09 (1/2)\n",
      "- Creating 100141 sequences for HTOL-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 100141/100141 [06:29<00:00, 257.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing machine HTOL-10 (2/2)\n",
      "- Creating 268184 sequences for HTOL-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 268184/268184 [17:03<00:00, 262.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sequences created: 368325\n",
      "Positive samples: 4\n",
      "Negative samples: 368321\n",
      "\n",
      "Step 4: Training and evaluating model\n",
      "\n",
      "Training and evaluating classifier:\n",
      "- Total samples: 368325\n",
      "- Training set size: 294660\n",
      "- Test set size: 73665\n",
      "Scaling features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost classifier...\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/artemmosnin/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model performance:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     73663\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00     73665\n",
      "   macro avg       0.50      0.50      0.50     73665\n",
      "weighted avg       1.00      1.00      1.00     73665\n",
      "\n",
      "\n",
      "Completed training for MEDIUM alerts\n",
      "\n",
      "==================================================\n",
      "Training model for HIGH alerts\n",
      "==================================================\n",
      "\n",
      "Step 3: Creating sequences\n",
      "\n",
      "Creating sequences...\n",
      "\n",
      "Processing machine HTOL-09 (1/2)\n",
      "- Creating 100141 sequences for HTOL-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences:   7%|▋         | 6521/100141 [00:24<05:55, 263.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m  \u001b[38;5;66;03m# 24 hours of data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_production_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m, in \u001b[0;36mtrain_production_models\u001b[0;34m(data_folder, output_dir, sequence_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m rf_predictor \u001b[38;5;241m=\u001b[39m SequenceAlertPredictor(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforest\u001b[39m\u001b[38;5;124m'\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39msequence_length)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining XGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mxgb_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining RF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m rf_predictor\u001b[38;5;241m.\u001b[39mtrain(data_folder)\n",
      "Cell \u001b[0;32mIn[72], line 148\u001b[0m, in \u001b[0;36mSequenceAlertPredictor.train\u001b[0;34m(self, folder)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_alert_type \u001b[38;5;241m=\u001b[39m alert_type\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 3: Creating sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 4: Training and evaluating model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m model, scaler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_and_evaluate_classifier(X, y)\n",
      "Cell \u001b[0;32mIn[72], line 58\u001b[0m, in \u001b[0;36mSequenceAlertPredictor.create_sequences\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_features:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length):\n\u001b[0;32m---> 58\u001b[0m         sequence_features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_t\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39miloc[j][feature]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alert_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malert_types:\n\u001b[1;32m     61\u001b[0m     sequence_features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target_row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir = \"production_sequence_models\"\n",
    "data_folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "sequence_length = 24  # 24 hours of data\n",
    "\n",
    "predictor = train_production_models(\n",
    "    data_folder=data_folder,\n",
    "    output_dir=output_dir,\n",
    "    sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training the models, we'll use a portion of the original data for testing\n",
    "def create_test_visualization(data_folder, predictor, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Creates visualizations using a portion of the training data as a test set.\n",
    "\n",
    "    Args:\n",
    "        data_folder: Path to the data folder\n",
    "        predictor: Trained ProductionSequencePredictor instance\n",
    "        test_size: Fraction of data to use for testing (default 0.2 = 20%)\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing data for visualization...\")\n",
    "\n",
    "    # Load the original data\n",
    "    dfs = []\n",
    "    for i in range(9, 9 + total_files):\n",
    "        file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_folder, file_name))\n",
    "        df['machine_id'] = f'HTOL-{i:02d}'\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "\n",
    "    # Sort by time and calculate features needed for prediction\n",
    "    combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "    # Calculate rolling statistics\n",
    "    combined_df['rolling_mean'] = combined_df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "        window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    combined_df['rolling_std'] = combined_df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "        window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "    # Calculate time since last alerts\n",
    "    for alert_type in predictor.alert_types:\n",
    "        combined_df[f'time_since_{alert_type}'] = combined_df.groupby('machine_id').apply(\n",
    "            lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "        combined_df[f'time_since_{alert_type}'] = combined_df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "\n",
    "    # Select a random machine for visualization\n",
    "    unique_machines = combined_df['machine_id'].unique()\n",
    "    test_machine = np.random.choice(unique_machines)\n",
    "    print(f\"\\nSelected machine {test_machine} for visualization\")\n",
    "\n",
    "    # Get the last 20% of data for the selected machine\n",
    "    machine_df = combined_df[combined_df['machine_id'] == test_machine].copy()\n",
    "    split_idx = int(len(machine_df) * (1 - test_size))\n",
    "    test_df = machine_df.iloc[split_idx:]\n",
    "\n",
    "    print(f\"Test set size: {len(test_df)} records\")\n",
    "    print(f\"Date range: {test_df['Time'].min()} to {test_df['Time'].max()}\")\n",
    "\n",
    "    # Create visualizations for each alert type\n",
    "    for alert_type in predictor.alert_types:\n",
    "        print(f\"\\nCreating visualization for {alert_type} alerts...\")\n",
    "        visualize_predictions(predictor, test_df, alert_type)\n",
    "\n",
    "        # Print some statistics\n",
    "        actual_alerts = sum(test_df['ALERT'] == alert_type)\n",
    "        print(f\"Number of actual {alert_type} alerts in test set: {actual_alerts}\")\n",
    "\n",
    "# Loading saved models and making predictions\n",
    "predictor = ProductionSequencePredictor.load_models(output_dir)\n",
    "\n",
    "# Then create visualizations using a portion of the training data\n",
    "create_test_visualization(data_folder, predictor, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
