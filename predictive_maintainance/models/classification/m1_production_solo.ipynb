{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlertPredictor:\n",
    "    def __init__(self, model_type='xgboost'):\n",
    "        \"\"\"\n",
    "        Initializes the AlertPredictor with the specified model type ('xgboost' or 'randomforest').\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH', ] # 'SIGMA']\n",
    "        self.features = ['ChlPrs',\n",
    "                        #  'hour',\n",
    "                        #  'day_of_week',\n",
    "                        #  'month',\n",
    "                        #  'is_weekend',\n",
    "                         'rolling_mean', 'rolling_std'] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses data from CSV files in the specified folder.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for i in range(9, 16):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            df = pd.read_csv(os.path.join(folder, file_name))\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineers features from the preprocessed data.\n",
    "        \"\"\"\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Calculate time since last alert for each type\n",
    "        for alert_type in self.alert_types:\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_for_classification(self, df, target_alert_type, prediction_window):\n",
    "        \"\"\"\n",
    "        Prepares the data for training the classification model.\n",
    "        \"\"\"\n",
    "        df['target'] = df.groupby('machine_id').apply(\n",
    "            lambda x: (x['ALERT'] == target_alert_type).rolling(window=prediction_window).max().shift(-prediction_window + 1)).reset_index(level=0,\n",
    "                                                                                                                                           drop=True)\n",
    "\n",
    "        X = df[self.features]\n",
    "        y = df['target'].fillna(0)  # Fill NaN with 0 (no alert)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Trains and evaluates the classification model.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            # XGBoost configuration for imbalanced classification\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  # Handle class imbalance\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                early_stopping_rounds=10,\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=0)\n",
    "        elif self.model_type == 'randomforest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'xgboost' or 'randomforest'.\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder, prediction_window=7):\n",
    "        \"\"\"\n",
    "        Trains the models for each alert type.\n",
    "        \"\"\"\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\nTraining model for {alert_type} alerts:\")\n",
    "            X, y = self.prepare_data_for_classification(df, alert_type, prediction_window)\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Makes predictions on new data.\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        for alert_type in self.alert_types:\n",
    "            X_new = new_data[self.features]\n",
    "            X_new_scaled = self.scalers[alert_type].transform(X_new)\n",
    "            alert_probability = self.models[alert_type].predict_proba(X_new_scaled)[0, 1]\n",
    "            predictions[alert_type] = alert_probability\n",
    "        return predictions\n",
    "\n",
    "    def visualize_alerts(self, df, target_alert_type, prediction_window, probability_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Visualizes actual alerts and high-risk periods.\n",
    "        \"\"\"\n",
    "        X = df[self.features]\n",
    "        X_scaled = self.scalers[target_alert_type].transform(X)\n",
    "\n",
    "        df['alert_probability'] = self.models[target_alert_type].predict_proba(X_scaled)[:, 1]\n",
    "        df['high_risk'] = df['alert_probability'] > probability_threshold\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        machines = df['machine_id'].unique()\n",
    "        n_machines = len(machines)\n",
    "\n",
    "        for i, machine_id in enumerate(machines):\n",
    "            machine_df = df[df['machine_id'] == machine_id]\n",
    "\n",
    "            # Plot actual alerts\n",
    "            alerts = machine_df[machine_df['ALERT'] == target_alert_type]\n",
    "            plt.scatter(alerts['Time'], [i - 0.2] * len(alerts), marker='o', s=100,\n",
    "                        label=f'Actual {target_alert_type} Alert' if i == 0 else \"\")\n",
    "\n",
    "            # Plot high-risk periods\n",
    "            high_risk_periods = machine_df[machine_df['high_risk']]\n",
    "            plt.scatter(high_risk_periods['Time'], [i + 0.2] * len(high_risk_periods), marker='x', s=100,\n",
    "                        label=f'High Risk Period ({target_alert_type})' if i == 0 else \"\")\n",
    "\n",
    "            plt.text(df['Time'].min(), i, machine_id, va='center', ha='right', fontweight='bold')\n",
    "\n",
    "        plt.yticks(range(n_machines), machines)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Machine ID')\n",
    "        plt.title(f'Actual Alerts vs High Risk Periods for {target_alert_type} Alerts')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAlertPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the production predictor that handles both XGBoost and Random Forest models.\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.features = [\n",
    "            'ChlPrs',\n",
    "            # 'hour',\n",
    "            # 'day_of_week',\n",
    "            # 'month',\n",
    "            # 'is_weekend',\n",
    "            'rolling_mean',\n",
    "            'rolling_std'\n",
    "        ] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def save_models(self, xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                   save_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Save trained models and scalers to disk.\n",
    "\n",
    "        Args:\n",
    "            xgb_predictor: Trained XGBoost AlertPredictor instance\n",
    "            rf_predictor: Trained Random Forest AlertPredictor instance\n",
    "            save_dir: Directory to save the models\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'features': self.features,\n",
    "            'alert_types': self.alert_types,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        joblib.dump(metadata, os.path.join(save_dir, 'metadata.joblib'))\n",
    "\n",
    "        # Save models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Save XGBoost models and scalers\n",
    "            joblib.dump(\n",
    "                xgb_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                xgb_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Save Random Forest models and scalers\n",
    "            joblib.dump(\n",
    "                rf_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                rf_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def load_models(self, load_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Load saved models and scalers from disk.\n",
    "\n",
    "        Args:\n",
    "            load_dir: Directory containing the saved models\n",
    "        \"\"\"\n",
    "        # Load metadata\n",
    "        metadata = joblib.load(os.path.join(load_dir, 'metadata.joblib'))\n",
    "        self.features = metadata['features']\n",
    "        self.alert_types = metadata['alert_types']\n",
    "\n",
    "        # Initialize nested dictionaries for models and scalers\n",
    "        self.models = {'xgboost': {}, 'randomforest': {}}\n",
    "        self.scalers = {'xgboost': {}, 'randomforest': {}}\n",
    "\n",
    "        # Load models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Load XGBoost\n",
    "            self.models['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Load Random Forest\n",
    "            self.models['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare features for prediction.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing at minimum 'Time', 'ChlPrs', and 'machine_id' columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with engineered features\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "\n",
    "        # Time-based features\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Time since last alert features\n",
    "        for alert_type in self.alert_types:\n",
    "            if 'ALERT' in df.columns:\n",
    "                df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                    lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "                ).reset_index(level=0, drop=True)\n",
    "                df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "            else:\n",
    "                # For new data without alert history, use a large value\n",
    "                df[f'time_since_{alert_type}'] = 168  # One week in hours\n",
    "\n",
    "        return df[self.features]\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Make predictions using the loaded models.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing required features\n",
    "            model_type: 'xgboost' or 'randomforest'\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing predictions for each machine and alert type\n",
    "        \"\"\"\n",
    "        if model_type not in ['xgboost', 'randomforest']:\n",
    "            raise ValueError(\"model_type must be 'xgboost' or 'randomforest'\")\n",
    "\n",
    "        # Prepare features\n",
    "        X = self.prepare_features(data)\n",
    "\n",
    "        # Make predictions for each machine and alert type\n",
    "        predictions = {}\n",
    "        for machine_id in data['machine_id'].unique():\n",
    "            machine_data = X[data['machine_id'] == machine_id]\n",
    "            machine_predictions = {}\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                # Scale the features\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(machine_data)\n",
    "\n",
    "                # Get prediction probabilities\n",
    "                probs = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "                # Store the average probability for this alert type\n",
    "                machine_predictions[alert_type] = float(probs.mean())\n",
    "\n",
    "            predictions[machine_id] = machine_predictions\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def save_trained_models(xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                       save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Convenience function to save trained models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.save_models(xgb_predictor, rf_predictor, save_dir)\n",
    "    print(f\"Models saved successfully to {save_dir}\")\n",
    "\n",
    "def load_production_predictor(load_dir: str) -> ProductionAlertPredictor:\n",
    "    \"\"\"\n",
    "    Convenience function to load saved models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.load_models(load_dir)\n",
    "    return production_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_47318/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m xgb_predictor \u001b[38;5;241m=\u001b[39m AlertPredictor(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mxgb_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m rf_predictor \u001b[38;5;241m=\u001b[39m AlertPredictor(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m rf_predictor\u001b[38;5;241m.\u001b[39mtrain(folder, prediction_window)\n",
      "Cell \u001b[0;32mIn[4], line 110\u001b[0m, in \u001b[0;36mAlertPredictor.train\u001b[0;34m(self, folder, prediction_window)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mTrains the models for each alert type.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_preprocess_data(folder)\n\u001b[0;32m--> 110\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alert_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malert_types:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m alerts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mAlertPredictor.engineer_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alert_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malert_types:\n\u001b[1;32m     49\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m x[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALERT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m alert_type][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreset_index(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 51\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_since_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malert_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_seconds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m  \u001b[38;5;66;03m# Convert to hours\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/indexes/accessors.py:132\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    131\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/indexes/extension.py:95\u001b[0m, in \u001b[0;36m_inherit_from_data.<locals>.method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use inplace with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mattr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/arrays/timedeltas.py:786\u001b[0m, in \u001b[0;36mTimedeltaArray.total_seconds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03mReturn total duration of each element expressed in seconds.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03mIndex([0.0, 86400.0, 172800.0, 259200.0, 345600.0], dtype='float64')\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m pps \u001b[38;5;241m=\u001b[39m periods_per_second(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_creso)\n\u001b[0;32m--> 786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_mask_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py:874\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._maybe_mask_results\u001b[0;34m(self, result, fill_value, convert)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m         fill_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m--> 874\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_isnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/numpy/_core/multiarray.py:1153\u001b[0m, in \u001b[0;36mputmask\u001b[0;34m(a, mask, values)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "prediction_window = 7  # days\n",
    "output_dir = \"production_models_solo\"\n",
    "\n",
    "# Train models\n",
    "xgb_predictor = AlertPredictor(model_type='xgboost')\n",
    "xgb_predictor.train(folder, prediction_window)\n",
    "\n",
    "rf_predictor = AlertPredictor(model_type='randomforest')\n",
    "rf_predictor.train(folder, prediction_window)\n",
    "\n",
    "# Save the trained models\n",
    "save_trained_models(xgb_predictor, rf_predictor, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_verdict(predictor, new_data):\n",
    "    LOW_thresh = 0.7\n",
    "    MEDIUM_thresh = 0.6\n",
    "    HIGH_thresh = 0.7\n",
    "    \"\"\"\n",
    "    Predicts alerts based on two models (XGBoost and Random Forest) and\n",
    "    returns a final verdict based on a probability_threshold. An alert is raised only\n",
    "    if both models predict the alert type with probability above the probability_threshold.\n",
    "\n",
    "    Args:\n",
    "      predictor: An object with a predict method that takes the data and\n",
    "                 model_type as arguments and returns predictions.\n",
    "      new_data: The input data for prediction.\n",
    "      probability_threshold: The probability probability_threshold for an alert to be considered valid.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary with machine IDs as keys and a list of alerts as values.\n",
    "    \"\"\"\n",
    "\n",
    "    xgb_predictions = predictor.predict(new_data, model_type='xgboost')\n",
    "    rf_predictions = predictor.predict(new_data, model_type='randomforest')\n",
    "    final_verdicts = {}\n",
    "\n",
    "    for machine_id, xgb_preds in xgb_predictions.items():\n",
    "        final_verdicts[machine_id] = []\n",
    "        rf_preds = rf_predictions[machine_id]\n",
    "\n",
    "        for alert_type in xgb_preds:\n",
    "            threshold = LOW_thresh if alert_type == \"LOW\" else MEDIUM_thresh if alert_type == \"MEDIUM\" else HIGH_thresh\n",
    "            if  statistics.mean([\n",
    "                # xgb_preds[alert_type],\n",
    "                rf_preds[alert_type]]) > threshold:\n",
    "                final_verdicts[machine_id].append(alert_type)\n",
    "\n",
    "    return final_verdicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for HTOL-09:\n",
      "XGBoost predictions:\n",
      "LOW: 0.941\n",
      "MEDIUM: 0.094\n",
      "HIGH: 0.196\n",
      "\n",
      "Random Forest predictions:\n",
      "LOW: 0.706\n",
      "MEDIUM: 0.494\n",
      "HIGH: 0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HTOL-09': ['LOW']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the production predictor\n",
    "predictor = load_production_predictor(output_dir)\n",
    "\n",
    "# Example new data\n",
    "new_data = pd.DataFrame({\n",
    "    'Time': ['2024-05-12 12:23:00', '2024-05-12 12:23:00', '2024-05-12 12:23:00', '2024-05-12 12:23:00', '2024-05-12 12:23:00'],\n",
    "    'machine_id': ['HTOL-09', 'HTOL-09', 'HTOL-09', 'HTOL-09',  'HTOL-09'],\n",
    "    'ChlPrs': [32, 34, 36, 39, 45]\n",
    "})\n",
    "\n",
    "# Get predictions from both models\n",
    "xgb_predictions = predictor.predict(new_data, model_type='xgboost')\n",
    "rf_predictions = predictor.predict(new_data, model_type='randomforest')\n",
    "\n",
    "# Print predictions\n",
    "for machine_id, predictions in xgb_predictions.items():\n",
    "    print(f\"\\nPredictions for {machine_id}:\")\n",
    "    print(\"XGBoost predictions:\")\n",
    "    for alert_type, prob in predictions.items():\n",
    "        print(f\"{alert_type}: {prob:.3f}\")\n",
    "\n",
    "    print(\"\\nRandom Forest predictions:\")\n",
    "    rf_preds = rf_predictions[machine_id]\n",
    "    for alert_type, prob in rf_preds.items():\n",
    "        print(f\"{alert_type}: {prob:.3f}\")\n",
    "\n",
    "get_final_verdict(predictor, new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
