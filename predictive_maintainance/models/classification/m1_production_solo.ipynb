{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlertPredictor:\n",
    "    def __init__(self, model_type='xgboost'):\n",
    "        \"\"\"\n",
    "        Initializes the AlertPredictor with the specified model type ('xgboost' or 'randomforest').\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH', ] # 'SIGMA']\n",
    "        self.features = ['ChlPrs',\n",
    "                        #  'hour',\n",
    "                        #  'day_of_week',\n",
    "                        #  'month',\n",
    "                        #  'is_weekend',\n",
    "                         'rolling_mean', 'rolling_std'] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def load_and_preprocess_data(self, folder):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses data from CSV files in the specified folder.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for i in range(9, 16):\n",
    "            file_name = f\"HTOL-{i:02d}_alerts.csv\"\n",
    "            df = pd.read_csv(os.path.join(folder, file_name))\n",
    "            df['machine_id'] = f'HTOL-{i:02d}'\n",
    "            dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df['Time'] = pd.to_datetime(combined_df['Time'])\n",
    "        combined_df = combined_df.sort_values(['machine_id', 'Time'])\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineers features from the preprocessed data.\n",
    "        \"\"\"\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Calculate rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Calculate time since last alert for each type\n",
    "        for alert_type in self.alert_types:\n",
    "            df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)).reset_index(level=0, drop=True)\n",
    "            df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_for_classification(self, df, target_alert_type, prediction_window):\n",
    "        \"\"\"\n",
    "        Prepares the data for training the classification model.\n",
    "        \"\"\"\n",
    "        df['target'] = df.groupby('machine_id').apply(\n",
    "            lambda x: (x['ALERT'] == target_alert_type).rolling(window=prediction_window).max().shift(-prediction_window + 1)).reset_index(level=0,\n",
    "                                                                                                                                           drop=True)\n",
    "\n",
    "        X = df[self.features]\n",
    "        y = df['target'].fillna(0)  # Fill NaN with 0 (no alert)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train_and_evaluate_classifier(self, X, y, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Trains and evaluates the classification model.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            # XGBoost configuration for imbalanced classification\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  # Handle class imbalance\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                early_stopping_rounds=10,\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=0)\n",
    "        elif self.model_type == 'randomforest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'xgboost' or 'randomforest'.\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return model, scaler\n",
    "\n",
    "    def train(self, folder, prediction_window=7):\n",
    "        \"\"\"\n",
    "        Trains the models for each alert type.\n",
    "        \"\"\"\n",
    "        df = self.load_and_preprocess_data(folder)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        for alert_type in self.alert_types:\n",
    "            print(f\"\\nTraining model for {alert_type} alerts:\")\n",
    "            X, y = self.prepare_data_for_classification(df, alert_type, prediction_window)\n",
    "            model, scaler = self.train_and_evaluate_classifier(X, y)\n",
    "            self.models[alert_type] = model\n",
    "            self.scalers[alert_type] = scaler\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Makes predictions on new data.\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        for alert_type in self.alert_types:\n",
    "            X_new = new_data[self.features]\n",
    "            X_new_scaled = self.scalers[alert_type].transform(X_new)\n",
    "            alert_probability = self.models[alert_type].predict_proba(X_new_scaled)[0, 1]\n",
    "            predictions[alert_type] = alert_probability\n",
    "        return predictions\n",
    "\n",
    "    def visualize_alerts(self, df, target_alert_type, prediction_window, probability_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Visualizes actual alerts and high-risk periods.\n",
    "        \"\"\"\n",
    "        X = df[self.features]\n",
    "        X_scaled = self.scalers[target_alert_type].transform(X)\n",
    "\n",
    "        df['alert_probability'] = self.models[target_alert_type].predict_proba(X_scaled)[:, 1]\n",
    "        df['high_risk'] = df['alert_probability'] > probability_threshold\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        machines = df['machine_id'].unique()\n",
    "        n_machines = len(machines)\n",
    "\n",
    "        for i, machine_id in enumerate(machines):\n",
    "            machine_df = df[df['machine_id'] == machine_id]\n",
    "\n",
    "            # Plot actual alerts\n",
    "            alerts = machine_df[machine_df['ALERT'] == target_alert_type]\n",
    "            plt.scatter(alerts['Time'], [i - 0.2] * len(alerts), marker='o', s=100,\n",
    "                        label=f'Actual {target_alert_type} Alert' if i == 0 else \"\")\n",
    "\n",
    "            # Plot high-risk periods\n",
    "            high_risk_periods = machine_df[machine_df['high_risk']]\n",
    "            plt.scatter(high_risk_periods['Time'], [i + 0.2] * len(high_risk_periods), marker='x', s=100,\n",
    "                        label=f'High Risk Period ({target_alert_type})' if i == 0 else \"\")\n",
    "\n",
    "            plt.text(df['Time'].min(), i, machine_id, va='center', ha='right', fontweight='bold')\n",
    "\n",
    "        plt.yticks(range(n_machines), machines)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Machine ID')\n",
    "        plt.title(f'Actual Alerts vs High Risk Periods for {target_alert_type} Alerts')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAlertPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the production predictor that handles both XGBoost and Random Forest models.\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.alert_types = ['LOW', 'MEDIUM', 'HIGH']\n",
    "        self.features = [\n",
    "            'ChlPrs',\n",
    "            # 'hour',\n",
    "            # 'day_of_week',\n",
    "            # 'month',\n",
    "            # 'is_weekend',\n",
    "            'rolling_mean',\n",
    "            'rolling_std'\n",
    "        ] + [f'time_since_{at}' for at in self.alert_types]\n",
    "\n",
    "    def save_models(self, xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                   save_dir: str = 'production_models') -> None:\n",
    "        \"\"\"\n",
    "        Save trained models and scalers to disk.\n",
    "\n",
    "        Args:\n",
    "            xgb_predictor: Trained XGBoost AlertPredictor instance\n",
    "            rf_predictor: Trained Random Forest AlertPredictor instance\n",
    "            save_dir: Directory to save the models\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'features': self.features,\n",
    "            'alert_types': self.alert_types,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        joblib.dump(metadata, os.path.join(save_dir, 'metadata.joblib'))\n",
    "\n",
    "        # Save models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Save XGBoost models and scalers\n",
    "            joblib.dump(\n",
    "                xgb_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                xgb_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Save Random Forest models and scalers\n",
    "            joblib.dump(\n",
    "                rf_predictor.models[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            joblib.dump(\n",
    "                rf_predictor.scalers[alert_type],\n",
    "                os.path.join(save_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def load_models(self, load_dir: str = 'production_models') -> None:\n",
    "        \"\"\"\n",
    "        Load saved models and scalers from disk.\n",
    "\n",
    "        Args:\n",
    "            load_dir: Directory containing the saved models\n",
    "        \"\"\"\n",
    "        # Load metadata\n",
    "        metadata = joblib.load(os.path.join(load_dir, 'metadata.joblib'))\n",
    "        self.features = metadata['features']\n",
    "        self.alert_types = metadata['alert_types']\n",
    "\n",
    "        # Initialize nested dictionaries for models and scalers\n",
    "        self.models = {'xgboost': {}, 'randomforest': {}}\n",
    "        self.scalers = {'xgboost': {}, 'randomforest': {}}\n",
    "\n",
    "        # Load models and scalers\n",
    "        for alert_type in self.alert_types:\n",
    "            # Load XGBoost\n",
    "            self.models['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['xgboost'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'xgboost_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "            # Load Random Forest\n",
    "            self.models['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_model.joblib')\n",
    "            )\n",
    "            self.scalers['randomforest'][alert_type] = joblib.load(\n",
    "                os.path.join(load_dir, f'randomforest_{alert_type.lower()}_scaler.joblib')\n",
    "            )\n",
    "\n",
    "    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare features for prediction.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing at minimum 'Time', 'ChlPrs', and 'machine_id' columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with engineered features\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "\n",
    "        # Time-based features\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "        df['hour'] = df['Time'].dt.hour\n",
    "        df['day_of_week'] = df['Time'].dt.dayofweek\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Rolling statistics\n",
    "        df['rolling_mean'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df['rolling_std'] = df.groupby('machine_id')['ChlPrs'].rolling(\n",
    "            window=24, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "        # Time since last alert features\n",
    "        for alert_type in self.alert_types:\n",
    "            if 'ALERT' in df.columns:\n",
    "                df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
    "                    lambda x: x['Time'] - x[x['ALERT'] == alert_type]['Time'].shift(1)\n",
    "                ).reset_index(level=0, drop=True)\n",
    "                df[f'time_since_{alert_type}'] = df[f'time_since_{alert_type}'].dt.total_seconds() / 3600\n",
    "            else:\n",
    "                # For new data without alert history, use a large value\n",
    "                df[f'time_since_{alert_type}'] = 168  # One week in hours\n",
    "\n",
    "        return df[self.features]\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Make predictions using the loaded models.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing required features\n",
    "            model_type: 'xgboost' or 'randomforest'\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing predictions for each machine and alert type\n",
    "        \"\"\"\n",
    "        if model_type not in ['xgboost', 'randomforest']:\n",
    "            raise ValueError(\"model_type must be 'xgboost' or 'randomforest'\")\n",
    "\n",
    "        # Prepare features\n",
    "        X = self.prepare_features(data)\n",
    "\n",
    "        # Make predictions for each machine and alert type\n",
    "        predictions = {}\n",
    "        for machine_id in data['machine_id'].unique():\n",
    "            machine_data = X[data['machine_id'] == machine_id]\n",
    "            machine_predictions = {}\n",
    "\n",
    "            for alert_type in self.alert_types:\n",
    "                # Scale the features\n",
    "                X_scaled = self.scalers[model_type][alert_type].transform(machine_data)\n",
    "\n",
    "                # Get prediction probabilities\n",
    "                probs = self.models[model_type][alert_type].predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "                # Store the average probability for this alert type\n",
    "                machine_predictions[alert_type] = float(probs.mean())\n",
    "\n",
    "            predictions[machine_id] = machine_predictions\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def save_trained_models(xgb_predictor: AlertPredictor, rf_predictor: AlertPredictor,\n",
    "                       save_dir: str = 'production_models') -> None:\n",
    "    \"\"\"\n",
    "    Convenience function to save trained models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.save_models(xgb_predictor, rf_predictor, save_dir)\n",
    "    print(f\"Models saved successfully to {save_dir}\")\n",
    "\n",
    "def load_production_predictor(load_dir: str = 'production_models') -> ProductionAlertPredictor:\n",
    "    \"\"\"\n",
    "    Convenience function to load saved models.\n",
    "    \"\"\"\n",
    "    production_predictor = ProductionAlertPredictor()\n",
    "    production_predictor.load_models(load_dir)\n",
    "    return production_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39763/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39763/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n",
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39763/1073905821.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f'time_since_{alert_type}'] = df.groupby('machine_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for LOW alerts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0np89js16x9b596pzk8m108c0000gn/T/ipykernel_39763/1073905821.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['target'] = df.groupby('machine_id').apply(\n"
     ]
    }
   ],
   "source": [
    "folder = \"../../../outlier_tolerance=5_grouping_time_window=200_anomaly_threshold=6_start_date=2022-01-01_end_date=2026-01-01\"\n",
    "prediction_window = 7  # days\n",
    "\n",
    "# Train models\n",
    "xgb_predictor = AlertPredictor(model_type='xgboost')\n",
    "xgb_predictor.train(folder, prediction_window)\n",
    "\n",
    "rf_predictor = AlertPredictor(model_type='randomforest')\n",
    "rf_predictor.train(folder, prediction_window)\n",
    "\n",
    "# Save the trained models\n",
    "save_trained_models(xgb_predictor, rf_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- day_of_week\n- hour\n- is_weekend\n- month\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-11-02 12:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTOL-09\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChlPrs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m      9\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Get predictions from both models\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxgboost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m rf_predictions \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(new_data, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 158\u001b[0m, in \u001b[0;36mProductionAlertPredictor.predict\u001b[0;34m(self, data, model_type)\u001b[0m\n\u001b[1;32m    154\u001b[0m machine_predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alert_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malert_types:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     X_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43malert_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmachine_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Get prediction probabilities\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model_type][alert_type]\u001b[38;5;241m.\u001b[39mpredict_proba(X_scaled)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mm/lib/python3.13/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- day_of_week\n- hour\n- is_weekend\n- month\n"
     ]
    }
   ],
   "source": [
    "# Load the production predictor\n",
    "predictor = load_production_predictor()\n",
    "\n",
    "# Example new data\n",
    "new_data = pd.DataFrame({\n",
    "    'Time': ['2024-11-02 12:00:00'],\n",
    "    'machine_id': ['HTOL-09'],\n",
    "    'ChlPrs': [5]\n",
    "})\n",
    "\n",
    "# Get predictions from both models\n",
    "xgb_predictions = predictor.predict(new_data, model_type='xgboost')\n",
    "rf_predictions = predictor.predict(new_data, model_type='randomforest')\n",
    "\n",
    "# Print predictions\n",
    "for machine_id, predictions in xgb_predictions.items():\n",
    "    print(f\"\\nPredictions for {machine_id}:\")\n",
    "    print(\"XGBoost predictions:\")\n",
    "    for alert_type, prob in predictions.items():\n",
    "        print(f\"{alert_type}: {prob:.3f}\")\n",
    "\n",
    "    print(\"\\nRandom Forest predictions:\")\n",
    "    rf_preds = rf_predictions[machine_id]\n",
    "    for alert_type, prob in rf_preds.items():\n",
    "        print(f\"{alert_type}: {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
